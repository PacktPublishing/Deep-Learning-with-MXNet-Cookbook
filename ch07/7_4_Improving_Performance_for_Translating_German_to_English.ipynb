{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbbe216b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "import gluonnlp as nlp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import sacremoses\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import io\n",
    "from importlib import reload\n",
    "\n",
    "# Local Libraries\n",
    "import nmt\n",
    "import dataprocessor\n",
    "import utils\n",
    "import nmt.transformer_hparams\n",
    "import transformer_model\n",
    "\n",
    "# Hyperparameters for Dataloaders and Training\n",
    "hparams = nmt.transformer_hparams\n",
    "\n",
    "# Seeds for reproducibility\n",
    "np.random.seed(100)\n",
    "random.seed(100)\n",
    "mx.random.seed(100)\n",
    "\n",
    "# CPU setup\n",
    "# ctx = mx.cpu()\n",
    "# Single GPU setup\n",
    "ctx = mx.gpu(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bac926f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WMT2016 Dataset\n",
    "# Train/Validation: Pre-processed\n",
    "# Test: Raw (for evaluation)\n",
    "\n",
    "# Dataset Parameters\n",
    "# src_lang, tgt_lang = \"de\", \"en\"\n",
    "src_lang, tgt_lang = \"en\", \"de\"\n",
    "src_max_len, tgt_max_len = 50, 50\n",
    "\n",
    "# wmt_train_text = nlp.data.WMT2016(\n",
    "#     \"train\",\n",
    "#     src_lang=src_lang,\n",
    "#     tgt_lang=tgt_lang)\n",
    "\n",
    "# wmt_test_text  = nlp.data.WMT2016(\n",
    "#     \"newstest2016\",\n",
    "#     src_lang=src_lang,\n",
    "#     tgt_lang=tgt_lang)\n",
    "\n",
    "wmt_train_text = nlp.data.WMT2016BPE(\n",
    "    \"train\",\n",
    "    src_lang=src_lang,\n",
    "    tgt_lang=tgt_lang)\n",
    "\n",
    "wmt_test_text  = nlp.data.WMT2016BPE(\n",
    "    \"newstest2016\",\n",
    "    src_lang=src_lang,\n",
    "    tgt_lang=tgt_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a3383d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train set: 4500966\n",
      "Length of test set : 2999\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of train set:\", len(wmt_train_text))\n",
    "# print(\"Length of val set  :\", len(wmt_val_text)) XXXX NOT CREATED YET\n",
    "print(\"Length of test set :\", len(wmt_test_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba744898-900f-4312-9dca-8fea430ec42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wmt_val_text = nlp.data.WMT2016(\n",
    "#     \"train\",\n",
    "#     src_lang=src_lang,\n",
    "#     tgt_lang=tgt_lang)\n",
    "\n",
    "wmt_val_text = nlp.data.WMT2016BPE(\n",
    "    \"train\",\n",
    "    src_lang=src_lang,\n",
    "    tgt_lang=tgt_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5018752-3b4c-4dc2-bdc8-db036d898597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation dataset generation (from training dataset)\n",
    "val_length = 3000\n",
    "\n",
    "wmt_val_text._data[0] = wmt_train_text._data[0][-val_length:]\n",
    "wmt_val_text._data[1] = wmt_train_text._data[1][-val_length:]\n",
    "wmt_val_text._length = val_length\n",
    "\n",
    "# Modify Training dataset to remove validation dataset\n",
    "# Mini Training set\n",
    "train_length = int(100e3)\n",
    "wmt_train_text._data[0] = wmt_train_text._data[0][:train_length]\n",
    "wmt_train_text._data[1] = wmt_train_text._data[1][:train_length]\n",
    "wmt_train_text._length = train_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "573ae218-5bbc-4a6b-9e2b-b4f3a13e32fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train set: 100000\n",
      "Length of val set  : 3000\n",
      "Length of test set : 2999\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of train set:\", len(wmt_train_text))\n",
    "print(\"Length of val set  :\", len(wmt_val_text))\n",
    "print(\"Length of test set :\", len(wmt_test_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a84f91c-364b-462a-9d98-81d92ee32f3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Vocab(size=36548, unk=\"<unk>\", reserved=\"['<eos>', '<bos>']\"),\n",
       " Vocab(size=36548, unk=\"<unk>\", reserved=\"['<eos>', '<bos>']\"))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wmt2016_src_vocab, wmt2016_tgt_vocab = wmt_train_text.src_vocab, wmt_train_text.tgt_vocab\n",
    "wmt2016_src_vocab, wmt2016_tgt_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edb426ce-a5d8-4a85-9095-082cef3f7cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Sequences (Val, Test)\n",
    "fetch_tgt_sentence = lambda src, tgt: tgt.split()\n",
    "val_tgt_sentences = list(wmt_val_text.transform(fetch_tgt_sentence))\n",
    "test_tgt_sentences = list(wmt_test_text.transform(fetch_tgt_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6276213e-4b04-4e24-9485-f161612eab92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset processing: clipping, tokenizing, indexing and adding of EOS (src/tgt) / BOS (tgt)\n",
    "wmt_train_processed = wmt_train_text.transform(\n",
    "    dataprocessor.TrainValDataTransform(\n",
    "        wmt2016_src_vocab,\n",
    "        wmt2016_tgt_vocab,\n",
    "        src_max_len,\n",
    "        tgt_max_len),\n",
    "    lazy=False)\n",
    "\n",
    "wmt_val_processed   = wmt_val_text.transform(\n",
    "    dataprocessor.TrainValDataTransform(\n",
    "        wmt2016_src_vocab,\n",
    "        wmt2016_tgt_vocab,\n",
    "        src_max_len,\n",
    "        tgt_max_len),\n",
    "    lazy=False)\n",
    "\n",
    "wmt_test_processed  = wmt_test_text.transform(\n",
    "    dataprocessor.TrainValDataTransform(\n",
    "        wmt2016_src_vocab,\n",
    "        wmt2016_tgt_vocab,\n",
    "        src_max_len,\n",
    "        tgt_max_len),\n",
    "    lazy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb5d994a-0a98-4c92-a97a-96ebbdb1e5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create Gluon Datasets\n",
    "# # Not needed for training, as training data will be sharded later\n",
    "# wmt_train_transformed = wmt_train_processed.transform(\n",
    "#     lambda src, tgt: (src, tgt, len(src), len(tgt)),\n",
    "#     lazy=False)\n",
    "\n",
    "# wmt_val_dataset = mx.gluon.data.SimpleDataset(\n",
    "#     [(ele[0], ele[1], len(ele[0]), len(ele[1]),i) for i, ele in enumerate(wmt_val_processed)])\n",
    "\n",
    "# wmt_test_dataset = mx.gluon.data.SimpleDataset(\n",
    "#     [(ele[0], ele[1], len(ele[0]), len(ele[1]), i) for i, ele in enumerate(wmt_test_processed)])\n",
    "\n",
    "def get_length_index_fn():\n",
    "    global idx\n",
    "    idx = 0\n",
    "    def transform(src, tgt):\n",
    "        global idx\n",
    "        result = (src, tgt, len(src), len(tgt), idx)\n",
    "        idx += 1\n",
    "        return result\n",
    "    return transform\n",
    "\n",
    "wmt_data_test_with_len = wmt_test_processed.transform(get_length_index_fn(), lazy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5703956b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updates for this recipe\n",
    "hparams.num_hidden = 512\n",
    "hparams.num_layers = 4\n",
    "hparams.dropout = 0.2\n",
    "hparams.num_buckets = 5\n",
    "hparams.lr = 0.001\n",
    "#hparams.lr = 0.0001\n",
    "#hparams.lr = 0.0003 achieves 21.44 test_bleu: qualitative evaluation didn't work\n",
    "#hparams.lr = 0.0001 achieves 19.66 test_bleu: qualitative evaluation worked\n",
    "hparams.clip = 5\n",
    "hparams.epochs = 12\n",
    "hparams.beam_size = 10\n",
    "hparams.lp_alpha = 1.0\n",
    "hparams.lp_k = 5\n",
    "\n",
    "hparams.max_length = 150\n",
    "hparams.batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32d3a2d7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wmt_train_transformed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Bucket scheme\u001b[39;00m\n\u001b[1;32m      9\u001b[0m bucket_scheme \u001b[38;5;241m=\u001b[39m nlp\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mExpWidthBucket(bucket_len_step\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.2\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m wmt_train_lengths \u001b[38;5;241m=\u001b[39m get_data_lengths(\u001b[43mwmt_train_transformed\u001b[49m)\n\u001b[1;32m     12\u001b[0m wmt_val_lengths \u001b[38;5;241m=\u001b[39m get_data_lengths(wmt_val_dataset)\n\u001b[1;32m     13\u001b[0m wmt_test_lengths \u001b[38;5;241m=\u001b[39m get_data_lengths(wmt_test_dataset)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wmt_train_transformed' is not defined"
     ]
    }
   ],
   "source": [
    "# Create Gluon Samplers and DataLoaders\n",
    "\n",
    "# Helper function for lengths\n",
    "def get_data_lengths(dataset):\n",
    "    get_lengths = lambda *args: (args[2], args[3])\n",
    "    return list(dataset.transform(get_lengths))\n",
    "\n",
    "# Bucket scheme\n",
    "bucket_scheme = nlp.data.ExpWidthBucket(bucket_len_step=1.2)\n",
    "\n",
    "wmt_train_lengths = get_data_lengths(wmt_train_transformed)\n",
    "wmt_val_lengths = get_data_lengths(wmt_val_dataset)\n",
    "wmt_test_lengths = get_data_lengths(wmt_test_dataset)\n",
    "\n",
    "train_batchify_fn = nlp.data.batchify.Tuple(\n",
    "    nlp.data.batchify.Pad(),\n",
    "    nlp.data.batchify.Pad(),\n",
    "    nlp.data.batchify.Stack(dtype='float32'),\n",
    "    nlp.data.batchify.Stack(dtype='float32'))\n",
    "\n",
    "test_batchify_fn = nlp.data.batchify.Tuple(\n",
    "    nlp.data.batchify.Pad(),\n",
    "    nlp.data.batchify.Pad(),\n",
    "    nlp.data.batchify.Stack(dtype='float32'),\n",
    "    nlp.data.batchify.Stack(dtype='float32'),\n",
    "    nlp.data.batchify.Stack())\n",
    "\n",
    "target_val_lengths = list(map(lambda x: x[-1], wmt_val_lengths))\n",
    "target_test_lengths = list(map(lambda x: x[-1], wmt_test_lengths))\n",
    "\n",
    "train_batch_sampler = nlp.data.FixedBucketSampler(\n",
    "    lengths=wmt_train_lengths,\n",
    "    batch_size=hparams.batch_size,\n",
    "    num_buckets=hparams.num_buckets,\n",
    "    ratio=0,\n",
    "    shuffle=True,\n",
    "    use_average_length=False,\n",
    "    num_shards=0,\n",
    "    bucket_scheme=bucket_scheme)\n",
    "    \n",
    "train_data_loader = nlp.data.ShardedDataLoader(\n",
    "    wmt_train_transformed,\n",
    "    batch_sampler=train_batch_sampler,\n",
    "    batchify_fn=train_batchify_fn,\n",
    "    num_workers=8)\n",
    "\n",
    "val_batch_sampler = nlp.data.FixedBucketSampler(\n",
    "    lengths=wmt_val_lengths,\n",
    "    batch_size=hparams.test_batch_size,\n",
    "    num_buckets=hparams.num_buckets,\n",
    "    ratio=0,\n",
    "    shuffle=False,\n",
    "    use_average_length=False,\n",
    "    bucket_scheme=bucket_scheme)\n",
    "\n",
    "val_data_loader = mx.gluon.data.DataLoader(\n",
    "    wmt_val_dataset,\n",
    "    batch_sampler=val_batch_sampler,\n",
    "    batchify_fn=test_batchify_fn,\n",
    "    num_workers=8)\n",
    "\n",
    "test_batch_sampler = nlp.data.FixedBucketSampler(\n",
    "    lengths=wmt_test_lengths,\n",
    "    batch_size=hparams.test_batch_size,\n",
    "    num_buckets=hparams.num_buckets,\n",
    "    ratio=0,\n",
    "    shuffle=False,\n",
    "    use_average_length=False,\n",
    "    bucket_scheme=bucket_scheme)\n",
    "\n",
    "test_data_loader = mx.gluon.data.DataLoader(\n",
    "    wmt_test_dataset,\n",
    "    batch_sampler=test_batch_sampler,\n",
    "    batchify_fn=test_batchify_fn,\n",
    "    num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d18b476-c560-4d02-ae47-1af6764f8fcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e284c7-7fa9-4646-b9ae-ad485f71d808",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc050290-7bb7-4e1a-b0bd-d416104ae335",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7096e874-b330-4639-bde3-b0d8dc55e0af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f19f320-4214-4c41-bf09-d304742b4a24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a1c512-1780-4f79-bbcf-a44c08ec2d6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9967f6e3-71ed-4375-ba53-2fb08920f76c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fffcbfb-d27f-4353-b468-377860b49aa3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ec11c9-5e38-4af5-bf59-ed863dea765b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "772d144e-0fa0-4f42-a0b0-bd0a406485da",
   "metadata": {},
   "source": [
    "## Training from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6433aa65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[15:54:20] ../src/base.cc:79: cuDNN lib mismatch: linked-against version 8600 != compiled-against version 8101.  Set MXNET_CUDNN_LIB_CHECKING=0 to quiet this warning.\n"
     ]
    }
   ],
   "source": [
    "# Transformer Model\n",
    "transformer_encoder, transformer_decoder, transformer_one_step_ahead_decoder = nlp.model.transformer.get_transformer_encoder_decoder(\n",
    "    hidden_size=hparams.num_hidden,\n",
    "    dropout=hparams.dropout,\n",
    "    num_layers=hparams.num_layers)\n",
    "\n",
    "transformer_model_ts = nlp.model.translation.NMTModel(\n",
    "    src_vocab=wmt_src_vocab,\n",
    "    tgt_vocab=wmt_tgt_vocab,\n",
    "    encoder=transformer_encoder,\n",
    "    decoder=transformer_decoder,\n",
    "    one_step_ahead_decoder=transformer_one_step_ahead_decoder,\n",
    "    #embed_size=hparams.num_hidden,\n",
    "    embed_size=hparams.num_units,\n",
    "    prefix='transformer_')\n",
    "\n",
    "transformer_model_ts.initialize(init=mx.init.Xavier(magnitude=1.0), ctx=ctx)\n",
    "transformer_model_ts.hybridize(static_alloc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d3ae74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translator (using model defined above)\n",
    "transformer_ts_translator = nmt.translation.BeamSearchTranslator(\n",
    "    model=transformer_model_ts,\n",
    "    beam_size=hparams.beam_size,\n",
    "    scorer=nlp.model.BeamSearchScorer(\n",
    "        alpha=hparams.lp_alpha,\n",
    "        K=hparams.lp_k),\n",
    "    # max_length=150)\n",
    "    max_length=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "076325eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "loss_function = nlp.loss.MaskedSoftmaxCELoss()\n",
    "loss_function.hybridize(static_alloc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0707abf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64fcf435dc10466a9cf71b29c98608ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c938d1b0f70a45d883d84edad758ed07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/785 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 100/785] loss=7.2770, ppl=1446.6195, gnorm=1.0628, throughput=39.78K wps, wc=640.02K\n",
      "[Epoch 0 Batch 200/785] loss=6.9850, ppl=1080.2670, gnorm=0.8397, throughput=44.56K wps, wc=682.64K\n",
      "[Epoch 0 Batch 300/785] loss=6.9788, ppl=1073.5958, gnorm=0.7445, throughput=42.69K wps, wc=601.62K\n",
      "[Epoch 0 Batch 400/785] loss=6.9852, ppl=1080.5719, gnorm=0.7487, throughput=43.33K wps, wc=622.22K\n",
      "[Epoch 0 Batch 500/785] loss=6.9809, ppl=1075.8973, gnorm=0.6450, throughput=43.51K wps, wc=615.55K\n",
      "[Epoch 0 Batch 600/785] loss=6.9887, ppl=1084.2783, gnorm=0.6042, throughput=41.96K wps, wc=572.96K\n",
      "[Epoch 0 Batch 700/785] loss=6.9689, ppl=1063.0504, gnorm=0.4530, throughput=43.56K wps, wc=644.42K\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16551091fc3a4d90857aead3dbda5690",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] valid Loss=8.8308, valid ppl=6842.0041, valid bleu=0.00\n",
      "Qualitative Evaluation: Translating from Vietnamese to English\n",
      "Expected translation:\n",
      "I like to read books.\n",
      "In Vietnamese (from Google Translate):\n",
      "Tôi thích đọc sách kỹ thuật.\n",
      "The English translation is:\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "518a846478f3419fb9c1a60f7c7b4352",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/785 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 Batch 100/785] loss=6.9312, ppl=1023.6773, gnorm=0.4437, throughput=40.27K wps, wc=627.10K\n",
      "[Epoch 1 Batch 200/785] loss=6.9542, ppl=1047.5575, gnorm=0.5122, throughput=40.71K wps, wc=603.48K\n",
      "[Epoch 1 Batch 300/785] loss=6.9507, ppl=1043.8606, gnorm=0.4681, throughput=42.08K wps, wc=646.65K\n",
      "[Epoch 1 Batch 400/785] loss=6.9542, ppl=1047.4990, gnorm=0.4005, throughput=41.86K wps, wc=651.48K\n",
      "[Epoch 1 Batch 500/785] loss=6.9468, ppl=1039.8311, gnorm=0.4278, throughput=41.91K wps, wc=653.35K\n",
      "[Epoch 1 Batch 600/785] loss=6.9363, ppl=1028.9366, gnorm=0.3741, throughput=41.50K wps, wc=636.45K\n",
      "[Epoch 1 Batch 700/785] loss=6.9520, ppl=1045.1875, gnorm=0.3871, throughput=40.51K wps, wc=596.07K\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e2aa324a326447eb805d41ea0b60e6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] valid Loss=8.8647, valid ppl=7077.7704, valid bleu=0.00\n",
      "Qualitative Evaluation: Translating from Vietnamese to English\n",
      "Expected translation:\n",
      "I like to read books.\n",
      "In Vietnamese (from Google Translate):\n",
      "Tôi thích đọc sách kỹ thuật.\n",
      "The English translation is:\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b392e706bd014378bf21f5cdd25cb94b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/785 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2 Batch 100/785] loss=6.9357, ppl=1028.3254, gnorm=0.4197, throughput=40.47K wps, wc=608.10K\n",
      "[Epoch 2 Batch 200/785] loss=6.9451, ppl=1038.0244, gnorm=0.4487, throughput=41.61K wps, wc=636.38K\n",
      "[Epoch 2 Batch 300/785] loss=6.9261, ppl=1018.5643, gnorm=0.4701, throughput=41.03K wps, wc=628.10K\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     40\u001b[0m grads \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mgrad(ctx) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m transformer_model\u001b[38;5;241m.\u001b[39mcollect_params()\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgrad_req \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnull\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 41\u001b[0m gnorm \u001b[38;5;241m=\u001b[39m \u001b[43mmx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgluon\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip_global_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m trainer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     44\u001b[0m src_wc \u001b[38;5;241m=\u001b[39m src_valid_length\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39masscalar()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/mxnet/gluon/utils.py:150\u001b[0m, in \u001b[0;36mclip_global_norm\u001b[0;34m(arrays, max_norm, check_isfinite)\u001b[0m\n\u001b[1;32m    148\u001b[0m total_norm \u001b[38;5;241m=\u001b[39m ndarray\u001b[38;5;241m.\u001b[39msqrt(total_norm)\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_isfinite:\n\u001b[0;32m--> 150\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misfinite(\u001b[43mtotal_norm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masscalar\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[1;32m    151\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    152\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnan or inf is detected. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    153\u001b[0m                         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClipping results will be undefined.\u001b[39m\u001b[38;5;124m'\u001b[39m), stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    154\u001b[0m scale \u001b[38;5;241m=\u001b[39m max_norm \u001b[38;5;241m/\u001b[39m (total_norm \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-8\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/mxnet/ndarray/ndarray.py:2590\u001b[0m, in \u001b[0;36mNDArray.asscalar\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2588\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe current array is not a scalar\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2589\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m-> 2590\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   2591\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2592\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masnumpy()[()]\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/mxnet/ndarray/ndarray.py:2568\u001b[0m, in \u001b[0;36mNDArray.asnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2551\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns a ``numpy.ndarray`` object with value copied from this array.\u001b[39;00m\n\u001b[1;32m   2552\u001b[0m \n\u001b[1;32m   2553\u001b[0m \u001b[38;5;124;03mExamples\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2565\u001b[0m \u001b[38;5;124;03m       [1, 1, 1]], dtype=int32)\u001b[39;00m\n\u001b[1;32m   2566\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2567\u001b[0m data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m-> 2568\u001b[0m check_call(\u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMXNDArraySyncCopyToCPU\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2569\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2570\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_as\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_void_p\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2571\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_size_t\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2572\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Let's train\n",
    "trainer = mx.gluon.Trainer(transformer_model_ts.collect_params(), hparams.optimizer, {'learning_rate': hparams.lr})\n",
    "\n",
    "best_valid_bleu = 0.0\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "valid_bleus  = []\n",
    "valid_perplexities = []\n",
    "\n",
    "for epoch_id in tqdm(range(hparams.epochs)):\n",
    "\n",
    "    log_loss = 0\n",
    "    log_denom = 0\n",
    "    log_avg_gnorm = 0\n",
    "    log_wc = 0\n",
    "    log_start_time = time.time()\n",
    "    \n",
    "    for batch_id, (src_seq, tgt_seq, src_valid_length, tgt_valid_length) in enumerate(tqdm(train_data_loader)):\n",
    "        \n",
    "        src_seq = src_seq.as_in_context(ctx)\n",
    "        tgt_seq = tgt_seq.as_in_context(ctx)\n",
    "        src_valid_length = src_valid_length.as_in_context(ctx)\n",
    "        tgt_valid_length = tgt_valid_length.as_in_context(ctx)\n",
    "        \n",
    "        with mx.autograd.record():\n",
    "            out, _ = transformer_model(\n",
    "                src_seq,\n",
    "                tgt_seq[:, :-1],\n",
    "                src_valid_length,\n",
    "                tgt_valid_length - 1)\n",
    "\n",
    "            loss = loss_function(out, tgt_seq[:, 1:], tgt_valid_length - 1).mean()\n",
    "            loss = loss * (tgt_seq.shape[1] - 1)\n",
    "            log_loss += loss * tgt_seq.shape[0]\n",
    "            log_denom += (tgt_valid_length - 1).sum()\n",
    "            loss = loss / (tgt_valid_length - 1).mean()\n",
    "            loss.backward()\n",
    "        \n",
    "        grads = [p.grad(ctx) for p in transformer_model.collect_params().values() if p.grad_req != 'null']\n",
    "        gnorm = mx.gluon.utils.clip_global_norm(grads, hparams.clip)\n",
    "        trainer.step(1)\n",
    "        \n",
    "        src_wc = src_valid_length.sum().asscalar()\n",
    "        tgt_wc = (tgt_valid_length - 1).sum().asscalar()\n",
    "        log_loss = log_loss.asscalar()\n",
    "        log_denom = log_denom.asscalar()\n",
    "        log_avg_gnorm += gnorm\n",
    "        log_wc += src_wc + tgt_wc\n",
    "        \n",
    "        train_loss = log_loss / log_denom\n",
    "        \n",
    "        if (batch_id + 1) % hparams.log_interval == 0:\n",
    "            wps = log_wc / (time.time() - log_start_time)\n",
    "            print(\"[Epoch {} Batch {}/{}] loss={:.4f}, ppl={:.4f}, gnorm={:.4f}, \"\n",
    "                         \"throughput={:.2f}K wps, wc={:.2f}K\"\n",
    "                         .format(epoch_id, batch_id + 1, len(train_data_loader),\n",
    "                                 train_loss,\n",
    "                                 np.exp(log_loss / log_denom),\n",
    "                                 log_avg_gnorm / hparams.log_interval,\n",
    "                                 wps / 1000, log_wc / 1000))\n",
    "            \n",
    "            log_start_time = time.time()\n",
    "            log_loss = 0\n",
    "            log_denom = 0\n",
    "            log_avg_gnorm = 0\n",
    "            log_wc = 0\n",
    "            \n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    valid_loss, valid_translation_out = nmt.utils.evaluate(\n",
    "        val_data_loader,\n",
    "        transformer_model,\n",
    "        transformer_translator,\n",
    "        loss_function,\n",
    "        wmt_tgt_vocab,\n",
    "        ctx)\n",
    "\n",
    "    valid_perplexity = np.exp(valid_loss)\n",
    "    valid_perplexities.append(valid_perplexity)\n",
    "    \n",
    "    valid_bleu_score, _, _, _, _ = nmt.bleu.compute_bleu([val_tgt_sentences], valid_translation_out)\n",
    "    print(\"[Epoch {}] valid Loss={:.4f}, valid ppl={:.4f}, valid bleu={:.2f}\"\n",
    "          .format(epoch_id, valid_loss, valid_perplexity, valid_bleu_score * 100))\n",
    "    \n",
    "    valid_losses.append(valid_loss)\n",
    "    valid_bleus.append(valid_bleu_score * 100)\n",
    "\n",
    "    if valid_bleu_score > best_valid_bleu:\n",
    "        best_valid_bleu = valid_bleu_score\n",
    "        print(\"Save best parameters to {}\".format(hparams.file_name))\n",
    "        transformer_model.save_parameters(hparams.file_name)\n",
    "    \n",
    "    if epoch_id + 1 >= (hparams.epochs * 2) // 3:\n",
    "        new_lr = trainer.learning_rate * hparams.lr_update_factor\n",
    "        print(\"Learning rate change to {}\".format(new_lr))\n",
    "        trainer.set_learning_rate(new_lr)\n",
    "        \n",
    "    print(\"Qualitative Evaluation: Translating from Vietnamese to English\")\n",
    "\n",
    "    expected_tgt_seq = \"I like to read books.\"\n",
    "    print(\"Expected translation:\")\n",
    "    print(expected_tgt_seq)\n",
    "    # From Google Translate\n",
    "    src_seq = \"Tôi thích đọc sách kỹ thuật.\"\n",
    "    print(\"In Vietnamese (from Google Translate):\")\n",
    "    print(src_seq)\n",
    "\n",
    "    translation_out = nmt.utils.translate_with_unk(\n",
    "    # translation_out = nmt.utils.translate(\n",
    "        transformer_translator,\n",
    "        src_seq,\n",
    "        wmt_src_vocab,\n",
    "        wmt_tgt_vocab,\n",
    "        ctx)\n",
    "\n",
    "    print(\"The English translation is:\")\n",
    "    print(\" \".join(translation_out[0]))\n",
    "\n",
    "if os.path.exists(hparams.file_name):\n",
    "    transformer_model.load_parameters(hparams.file_name)\n",
    "\n",
    "valid_loss, valid_translation_out = nmt.utils.evaluate(\n",
    "    val_data_loader,\n",
    "    transformer_model,\n",
    "    transformer_translator,\n",
    "    loss_function,\n",
    "    wmt_tgt_vocab,\n",
    "    ctx)\n",
    "\n",
    "valid_bleu_score, _, _, _, _ = nmt.bleu.compute_bleu([val_tgt_sentences], valid_translation_out)\n",
    "print(\"Best model valid Loss={:.4f}, valid ppl={:.4f}, valid bleu={:.2f}\"\n",
    "      .format(valid_loss, np.exp(valid_loss), valid_bleu_score * 100))\n",
    "\n",
    "test_loss, test_translation_out = nmt.utils.evaluate(\n",
    "    test_data_loader,\n",
    "    transformer_model,\n",
    "    transformer_translator,\n",
    "    loss_function,\n",
    "    wmt_tgt_vocab,\n",
    "    ctx)\n",
    "\n",
    "test_bleu_score, _, _, _, _ = nmt.bleu.compute_bleu([test_tgt_sentences], test_translation_out)\n",
    "print(\"Best model test Loss={:.4f}, test ppl={:.4f}, test bleu={:.2f}'\"\n",
    "      .format(test_loss, np.exp(test_loss), test_bleu_score * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cded54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot losses and validation accuracy\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(range(0, hparams.epochs), valid_losses, label=\"Validation Loss\")\n",
    "plt.plot(range(0, hparams.epochs), train_losses, label=\"Training Loss\")\n",
    "plt.plot(range(0, hparams.epochs), valid_perplexities, label=\"Validation Perplexity\")\n",
    "plt.plot(range(0, hparams.epochs), valid_bleus, label=\"Validation BLEU\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.title(\"Losses / Perplexity / BLEU\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe7107c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Qualitative Evaluation: Translating from Vietnamese to English\")\n",
    "\n",
    "expected_tgt_seq = \"I like to read books.\"\n",
    "print(\"Expected translation:\")\n",
    "print(expected_tgt_seq)\n",
    "# From Google Translate\n",
    "src_seq = \"Tôi thích đọc sách kỹ thuật.\"\n",
    "print(\"In Vietnamese (from Google Translate):\")\n",
    "print(src_seq)\n",
    "\n",
    "translation_out = nmt.utils.translate_with_unk(\n",
    "# translation_out = nmt.utils.translate(\n",
    "    transformer_translator,\n",
    "    src_seq,\n",
    "    wmt_src_vocab,\n",
    "    wmt_tgt_vocab,\n",
    "    ctx)\n",
    "\n",
    "print(\"The English translation is:\")\n",
    "print(\" \".join(translation_out[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d105b502-af67-4abb-a87b-dcbd7593e52a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005575ab-7fc6-4e4b-8f25-c22dc6e076a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3c36c8-7278-4494-a30c-bc45f455847d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892251e4-5f46-4c79-a5ca-87e158ca9b82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7c4a00-c804-4b19-8327-26d3b5c93eff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21ecd91-39c0-4ae3-953e-6ada02a3c634",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160d0dbd-1e49-4b26-bd95-184eff98f46b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee4dc2a-2141-483b-974e-cea790a66e02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9fc93a-891a-4992-afaa-6d294d357719",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b97b6f-12d4-4785-9d2f-032bd8ca9e94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57a4902-aeef-420a-b48d-515c48606658",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd28927-b142-4808-984a-883b44c5b17c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "68703d9d-598d-4617-af25-b45108a4569a",
   "metadata": {},
   "source": [
    "## Pre-Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60a9f44e-a7b3-4241-9caa-87f970b50675",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andreto/.local/lib/python3.8/site-packages/gluonnlp/vocab/vocab.py:590: UserWarning: Detected a corrupted index in the deserialize vocabulary. For versions before GluonNLP v0.7 the index is corrupted by specifying the same token for different special purposes, for example eos_token == padding_token. Deserializing the vocabulary nevertheless.\n",
      "  warnings.warn(\n",
      "[14:05:03] ../src/base.cc:79: cuDNN lib mismatch: linked-against version 8600 != compiled-against version 8101.  Set MXNET_CUDNN_LIB_CHECKING=0 to quiet this warning.\n"
     ]
    }
   ],
   "source": [
    "wmt_model_name = 'transformer_en_de_512'\n",
    "wmt_transformer_model_pt, wmt_src_vocab, wmt_tgt_vocab = nlp.model.get_model(\n",
    "    wmt_model_name,\n",
    "    dataset_name='WMT2014',\n",
    "    pretrained=True,\n",
    "    ctx=ctx)\n",
    "\n",
    "wmt_transformer_model_pt.hybridize(static_alloc=True)\n",
    "\n",
    "model_filename_pt = \"transformer_en_de_512_pt.params\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97c48cc0-9bcd-449a-81a1-5346cc4a950e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample BPE tokens: \"('Obama receives Net@@ any@@ ah@@ u', 'Obama empfängt Net@@ any@@ ah@@ u')\"\n",
      "Sample raw text: \"('Obama receives Netanyahu', 'Obama empfängt Netanyahu')\"\n",
      "Sample target sentence: \"Obama empfängt Netanyahu\"\n"
     ]
    }
   ],
   "source": [
    "# Reload data with model vocab\n",
    "src_lang, tgt_lang = \"en\", \"de\"\n",
    "\n",
    "wmt_data_test = nlp.data.WMT2016BPE(\n",
    "    'newstest2016',\n",
    "    src_lang=src_lang,\n",
    "    tgt_lang=tgt_lang)\n",
    "print('Sample BPE tokens: \"{}\"'.format(wmt_data_test[0]))\n",
    "\n",
    "wmt_test_text = nlp.data.WMT2016(\n",
    "    'newstest2016',\n",
    "    src_lang=src_lang,\n",
    "    tgt_lang=tgt_lang)\n",
    "print('Sample raw text: \"{}\"'.format(wmt_test_text[0]))\n",
    "\n",
    "wmt_test_tgt_sentences = wmt_test_text.transform(lambda src, tgt: tgt)\n",
    "print('Sample target sentence: \"{}\"'.format(wmt_test_tgt_sentences[0]))\n",
    "\n",
    "# wmt_src_vocab, wmt_tgt_vocab = nmt.utils.create_vocab(wmt_test_text)\n",
    "src_max_len, tgt_max_len = 50, 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a094679e-1278-4e41-bd10-d4a04f49fa6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing WMT2016 with WMT2014 model vocab\n",
    "wmt_dataset_processed = wmt_data_test.transform(\n",
    "    dataprocessor.TrainValDataTransform(\n",
    "        wmt_src_vocab,\n",
    "        wmt_tgt_vocab,\n",
    "        src_max_len,\n",
    "        tgt_max_len),\n",
    "    lazy=False)\n",
    "\n",
    "def get_length_index_fn():\n",
    "    global idx\n",
    "    idx = 0\n",
    "    def transform(src, tgt):\n",
    "        global idx\n",
    "        result = (src, tgt, len(src), len(tgt), idx)\n",
    "        idx += 1\n",
    "        return result\n",
    "    return transform\n",
    "\n",
    "wmt_data_test_with_len = wmt_dataset_processed.transform(get_length_index_fn(), lazy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77f06ffc-b6ba-4ce1-94d5-666e7ab291cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "wmt_test_batchify_fn = nlp.data.batchify.Tuple(\n",
    "    nlp.data.batchify.Pad(pad_val=0),\n",
    "    nlp.data.batchify.Pad(pad_val=0),\n",
    "    nlp.data.batchify.Stack(dtype='float32'),\n",
    "    nlp.data.batchify.Stack(dtype='float32'),\n",
    "    nlp.data.batchify.Stack())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c876202-7121-454d-be66-23c8b6885a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FixedBucketSampler:\n",
      "  sample_num=2999, batch_num=390\n",
      "  key=[7, 9, 12, 15, 18, 23, 28, 35, 43, 52]\n",
      "  cnt=[26, 67, 169, 275, 302, 435, 421, 442, 355, 507]\n",
      "  batch_size=[36, 28, 21, 17, 14, 11, 9, 7, 6, 4]\n"
     ]
    }
   ],
   "source": [
    "wmt_bucket_scheme = nlp.data.ExpWidthBucket(bucket_len_step=1.2)\n",
    "wmt_test_batch_sampler = nlp.data.FixedBucketSampler(\n",
    "    lengths=wmt_data_test_with_len.transform(lambda src, tgt, src_len, tgt_len, idx: tgt_len), # target length\n",
    "    use_average_length=True, # control the element lengths (i.e. number of tokens) to be about the same\n",
    "    bucket_scheme=wmt_bucket_scheme,\n",
    "    batch_size=hparams.batch_size)\n",
    "print(wmt_test_batch_sampler.stats())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bfcc620-8e13-43fa-b637-84d42c8f185f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "390"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_workers=0\n",
    "\n",
    "wmt_test_data_loader = mx.gluon.data.DataLoader(\n",
    "    wmt_data_test_with_len,\n",
    "    batch_sampler=wmt_test_batch_sampler,\n",
    "    batchify_fn=wmt_test_batchify_fn,\n",
    "    num_workers=num_workers)\n",
    "len(wmt_test_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0609e283-d724-4339-83bf-44878ba58ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_pt_translator = nmt.translation.BeamSearchTranslator(\n",
    "    model=wmt_transformer_model_pt,\n",
    "    beam_size=hparams.beam_size,\n",
    "    scorer=nlp.model.BeamSearchScorer(alpha=hparams.lp_alpha, K=hparams.lp_k),\n",
    "    max_length=200)\n",
    "\n",
    "wmt_test_loss_function = nlp.loss.MaskedSoftmaxCELoss()\n",
    "wmt_test_loss_function.hybridize()\n",
    "\n",
    "wmt_detokenizer = nlp.data.SacreMosesDetokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7c44c89-b581-4934-82df-feede0093b5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9bcce3e66984c3cafc651f42edc51f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/390 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WMT16 test loss: 1.59; test bleu score: 29.76\n"
     ]
    }
   ],
   "source": [
    "reload(transformer_model)\n",
    "\n",
    "wmt_test_loss, wmt_test_translation_out = transformer_model.evaluate(\n",
    "    wmt_transformer_model_pt,\n",
    "    wmt_test_data_loader,\n",
    "    wmt_test_loss_function,\n",
    "    transformer_pt_translator,\n",
    "    wmt_tgt_vocab,\n",
    "    wmt_detokenizer,\n",
    "    ctx)\n",
    "\n",
    "wmt_test_bleu_score, _, _, _, _ = nmt.bleu.compute_bleu(\n",
    "    [wmt_test_tgt_sentences],\n",
    "    wmt_test_translation_out,\n",
    "    tokenized=False,\n",
    "    tokenizer=\"13a\",\n",
    "    split_compound_word=False,\n",
    "    bpe=False)\n",
    "\n",
    "print('WMT16 test loss: %.2f; test bleu score: %.2f'\n",
    "      %(wmt_test_loss, wmt_test_bleu_score * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52860ec-55eb-43ee-b1ea-485d6271e54e",
   "metadata": {},
   "source": [
    "## Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccfa6e3-8cfd-4697-bfe3-a093e1059dca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf0559c-8be6-46ef-87dd-fd3388e979e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aef9ddb-3f60-4468-aa58-14d99c322e3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f597306-4abb-4464-b78c-4ef0ba393a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TEMP XXXXXXX #####\n",
    "#### LOAD TRAIN VAL DATASETS FOR TRAINING XXXXXXX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d98cf641-569a-4b27-a5ac-ee49abaccb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload data with model vocab\n",
    "src_lang, tgt_lang = \"en\", \"de\"\n",
    "\n",
    "wmt2016_train_data = nlp.data.WMT2016BPE(\n",
    "    'train',\n",
    "    src_lang=src_lang,\n",
    "    tgt_lang=tgt_lang)\n",
    "\n",
    "wmt2016_val_data = nlp.data.WMT2016BPE(\n",
    "    'train',\n",
    "    src_lang=src_lang,\n",
    "    tgt_lang=tgt_lang)\n",
    "\n",
    "wmt2016_test_data = nlp.data.WMT2016BPE(\n",
    "    'newstest2016',\n",
    "    src_lang=src_lang,\n",
    "    tgt_lang=tgt_lang)\n",
    "\n",
    "# Text samples not required for train/val\n",
    "# wmt2016_train_text = nlp.data.WMT2016(\n",
    "#     'train',\n",
    "#     src_lang=src_lang,\n",
    "#     tgt_lang=tgt_lang)\n",
    "\n",
    "# wmt2016_val_text = nlp.data.WMT2016(\n",
    "#     'train',\n",
    "#     src_lang=src_lang,\n",
    "#     tgt_lang=tgt_lang)\n",
    "\n",
    "# wmt2016_src_vocab, wmt2016_tgt_vocab = nmt.utils.create_vocab(wmt2016_test_text)\n",
    "src_max_len, tgt_max_len = 50, 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "072e0714-8c25-4a93-aaf9-56330b269c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation dataset generation (from training dataset)\n",
    "val_length = 3000\n",
    "\n",
    "wmt2016_val_data._data[0] = wmt2016_train_data._data[0][-val_length:]\n",
    "wmt2016_val_data._data[1] = wmt2016_train_data._data[1][-val_length:]\n",
    "wmt2016_val_data._length = val_length\n",
    "\n",
    "# Modify Training dataset to remove validation dataset\n",
    "# Mini Training set\n",
    "train_length = int(3000)\n",
    "wmt2016_train_data._data[0] = wmt2016_train_data._data[0][:train_length]\n",
    "wmt2016_train_data._data[1] = wmt2016_train_data._data[1][:train_length]\n",
    "wmt2016_train_data._length = train_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3bf0e02b-968b-48ed-bf53-1731f7a2b482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target sentences for validation and test\n",
    "fetch_tgt_sentence = lambda src, tgt: tgt.split()\n",
    "wmt2016_val_tgt_sentences = wmt2016_val_data.transform(fetch_tgt_sentence)\n",
    "wmt2016_test_tgt_sentences = wmt2016_test_data.transform(fetch_tgt_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "762f2e47-fc01-4814-be36-9118743610c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(transformer_model)\n",
    "\n",
    "# Pre-processing WMT2016 with WMT2014 model vocab\n",
    "wmt_transform_fn = dataprocessor.TrainValDataTransform(\n",
    "    wmt_src_vocab,\n",
    "    wmt_tgt_vocab,\n",
    "    src_max_len,\n",
    "    tgt_max_len)\n",
    "\n",
    "wmt2016_train_data_processed = wmt2016_train_data.transform(\n",
    "    wmt_transform_fn,\n",
    "    lazy=False)\n",
    "\n",
    "wmt2016_val_data_processed = wmt2016_val_data.transform(\n",
    "    wmt_transform_fn,\n",
    "    lazy=False)\n",
    "\n",
    "wmt2016_test_data_processed = wmt2016_test_data.transform(\n",
    "    wmt_transform_fn,\n",
    "    lazy=False)\n",
    "\n",
    "wmt2016_train_data_lengths = transformer_model.get_data_lengths(wmt2016_train_data_processed)\n",
    "wmt2016_val_data_lengths = transformer_model.get_data_lengths(wmt2016_val_data_processed)\n",
    "wmt2016_test_data_lengths = transformer_model.get_data_lengths(wmt2016_test_data_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ddd10888-0f97-4d47-a2c1-d85b9a336b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Lengths to the datasets and indexes for validation and test\n",
    "wmt2016_train_data_len_processed = wmt2016_train_data_processed.transform(lambda src, tgt: (src, tgt, len(src), len(tgt)), lazy=False)\n",
    "wmt2016_val_data_len_processed = wmt2016_val_data_processed.transform(transformer_model.get_length_index_fn(), lazy=False)\n",
    "wmt2016_test_data_len_processed = wmt2016_test_data_processed.transform(transformer_model.get_length_index_fn(), lazy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "55e1a647-2188-43ba-bf1c-b64308f3203e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batchify_fn = nlp.data.batchify.Tuple(\n",
    "    nlp.data.batchify.Pad(pad_val=0),\n",
    "    nlp.data.batchify.Pad(pad_val=0),\n",
    "    nlp.data.batchify.Stack(dtype='float32'),\n",
    "    nlp.data.batchify.Stack(dtype='float32'))\n",
    "\n",
    "val_batchify_fn = nlp.data.batchify.Tuple(\n",
    "    nlp.data.batchify.Pad(pad_val=0),\n",
    "    nlp.data.batchify.Pad(pad_val=0),\n",
    "    nlp.data.batchify.Stack(dtype='float32'),\n",
    "    nlp.data.batchify.Stack(dtype='float32'),\n",
    "    nlp.data.batchify.Stack())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4713eafc-b419-4a37-815a-b349610c824a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FixedBucketSampler:\n",
      "  sample_num=3000, batch_num=445\n",
      "  key=[(9, 10), (16, 17), (26, 27), (37, 38), (51, 52)]\n",
      "  cnt=[139, 468, 775, 697, 921]\n",
      "  batch_size=[27, 16, 10, 7, 4]\n",
      "FixedBucketSampler:\n",
      "  sample_num=3000, batch_num=484\n",
      "  key=[(9, 10), (17, 18), (26, 27), (38, 39), (51, 52)]\n",
      "  cnt=[82, 357, 615, 853, 1093]\n",
      "  batch_size=[28, 16, 10, 7, 4]\n",
      "FixedBucketSampler:\n",
      "  sample_num=2999, batch_num=369\n",
      "  key=[(10, 11), (18, 19), (27, 28), (38, 39), (51, 52)]\n",
      "  cnt=[155, 694, 782, 663, 705]\n",
      "  batch_size=[25, 15, 10, 7, 5]\n"
     ]
    }
   ],
   "source": [
    "reload(transformer_model)\n",
    "\n",
    "bucket_scheme = nlp.data.ExpWidthBucket(bucket_len_step=1.2)\n",
    "\n",
    "wmt2016_train_batch_sampler = nlp.data.FixedBucketSampler(\n",
    "    lengths=wmt2016_train_data_lengths,\n",
    "    use_average_length=True, # control the element lengths (i.e. number of tokens) to be about the same\n",
    "    num_buckets=hparams.num_buckets,\n",
    "    bucket_scheme=bucket_scheme,\n",
    "    batch_size=hparams.batch_size,\n",
    "    shuffle=True)\n",
    "print(wmt2016_train_batch_sampler.stats())\n",
    "\n",
    "wmt2016_val_batch_sampler = nlp.data.FixedBucketSampler(\n",
    "    lengths=wmt2016_val_data_lengths,\n",
    "    use_average_length=True, # control the element lengths (i.e. number of tokens) to be about the same\n",
    "    num_buckets=hparams.num_buckets,\n",
    "    bucket_scheme=bucket_scheme,\n",
    "    batch_size=hparams.batch_size,\n",
    "    shuffle=False)\n",
    "print(wmt2016_val_batch_sampler.stats())\n",
    "\n",
    "wmt2016_test_batch_sampler = nlp.data.FixedBucketSampler(\n",
    "    lengths=wmt2016_test_data_lengths,\n",
    "    use_average_length=True, # control the element lengths (i.e. number of tokens) to be about the same\n",
    "    num_buckets=hparams.num_buckets,\n",
    "    bucket_scheme=bucket_scheme,\n",
    "    batch_size=hparams.batch_size,\n",
    "    shuffle=False)\n",
    "print(wmt2016_test_batch_sampler.stats())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7901aeb7-276c-4d66-8b5e-2a04058efd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 0\n",
    "\n",
    "wmt2016_train_data_loader = nlp.data.ShardedDataLoader(\n",
    "    wmt2016_train_data_len_processed,\n",
    "    batch_sampler=wmt2016_train_batch_sampler,\n",
    "    batchify_fn=train_batchify_fn,\n",
    "    num_workers=num_workers)\n",
    "\n",
    "wmt2016_val_data_loader = nlp.data.ShardedDataLoader(\n",
    "    wmt2016_val_data_len_processed,\n",
    "    batch_sampler=wmt2016_val_batch_sampler,\n",
    "    batchify_fn=val_batchify_fn,\n",
    "    num_workers=num_workers)\n",
    "\n",
    "wmt2016_test_data_loader = nlp.data.ShardedDataLoader(\n",
    "    wmt2016_test_data_len_processed,\n",
    "    batch_sampler=wmt2016_test_batch_sampler,\n",
    "    batchify_fn=val_batchify_fn,\n",
    "    num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "be6815f6-4baa-407c-b00a-a3241bb7382c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wmt_transformer_model_tl = wmt_transformer_model_pt\n",
    "\n",
    "# # Freeze Layers (keeping track of the updated parameters)\n",
    "# updated_params = []\n",
    "# for param in wmt_transformer_model_tl.collect_params().values():\n",
    "#     if param.grad_req == \"write\":\n",
    "#         param.grad_req = \"null\"\n",
    "#         updated_params += [param.name]\n",
    "\n",
    "# # Re-enable gradients for last layer\n",
    "# for param in wmt_transformer_model_tl.tgt_proj.collect_params().values():\n",
    "#     if param in updated_params:\n",
    "#         param.grad_req = \"write\"\n",
    "\n",
    "# What if we don't overwrite this?\n",
    "# wmt_transformer_model_tl.tgt_proj = mx.gluon.nn.Dense(units=len(wmt_tgt_vocab), flatten=False, prefix='tgt_proj_')\n",
    "# wmt_transformer_model_tl.tgt_proj.initialize(ctx=ctx)\n",
    "\n",
    "# wmt_transformer_model_tl.hybridize(static_alloc=True)\n",
    "\n",
    "model_filename_tl = \"transformer_en_de_512_tl.params\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "064584ce-99d6-4323-90f7-7a2f646bc7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "wmt_translator_tl = nmt.translation.BeamSearchTranslator(\n",
    "    model=wmt_transformer_model_tl,\n",
    "    beam_size=hparams.beam_size,\n",
    "    scorer=nlp.model.BeamSearchScorer(alpha=hparams.lp_alpha, K=hparams.lp_k),\n",
    "    max_length=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "225c054a-e820-4947-9d75-cec9c15a3525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3be279760a54a948245fab0c5c90e9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27bfa852c432457f8058588fc1e0a8e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/445 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 100/445] loss=0.9405, ppl=2.5614, gnorm=5.0395, throughput=6.79K wps, wc=37.98K\n",
      "[Epoch 0 Batch 200/445] loss=0.9812, ppl=2.6677, gnorm=5.0645, throughput=6.54K wps, wc=38.19K\n",
      "[Epoch 0 Batch 300/445] loss=1.0089, ppl=2.7426, gnorm=5.2692, throughput=6.61K wps, wc=38.13K\n",
      "[Epoch 0 Batch 400/445] loss=1.0695, ppl=2.9138, gnorm=5.6947, throughput=6.76K wps, wc=37.63K\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f8e6ade7dba4752821c393933b9da9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/484 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AssertionError",
     "evalue": "references and translation should have format of list(list(str)) and list(str), respectively, when tokenized is False.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 95\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# Validation step\u001b[39;00m\n\u001b[1;32m     86\u001b[0m wmt2016_valid_loss, wmt2016_valid_translation_out \u001b[38;5;241m=\u001b[39m transformer_model\u001b[38;5;241m.\u001b[39mevaluate(\n\u001b[1;32m     87\u001b[0m     wmt_transformer_model_tl,\n\u001b[1;32m     88\u001b[0m     wmt2016_val_data_loader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     92\u001b[0m     wmt_detokenizer,\n\u001b[1;32m     93\u001b[0m     ctx)\n\u001b[0;32m---> 95\u001b[0m wmt2016_valid_bleu_score, _, _, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mnmt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbleu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_bleu\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mwmt2016_val_tgt_sentences\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwmt2016_valid_translation_out\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m13a\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_compound_word\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbpe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m wmt2016_valid_perplexity \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(wmt2016_valid_loss)\n\u001b[1;32m    104\u001b[0m wmt_2016_valid_perplexities\u001b[38;5;241m.\u001b[39mappend(wmt2016_valid_perplexity)\n",
      "File \u001b[0;32m~/code/public/Deep-Learning-with-MXNet-Cookbook/ch07/nmt/bleu.py:209\u001b[0m, in \u001b[0;36mcompute_bleu\u001b[0;34m(reference_corpus_list, translation_corpus, tokenized, tokenizer, max_n, smooth, lower_case, bpe, split_compound_word)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(reference_corpus_list[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m], LIST_TYPES) \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    205\u001b[0m            \u001b[38;5;28misinstance\u001b[39m(translation_corpus[\u001b[38;5;241m0\u001b[39m], LIST_TYPES), \\\n\u001b[1;32m    206\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreferences and translation should have format of list of list(list(str)) \u001b[39m\u001b[38;5;124m'\u001b[39m \\\n\u001b[1;32m    207\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mand list(list(str)), respectively, when tokenized is True.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 209\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(reference_corpus_list[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m], six\u001b[38;5;241m.\u001b[39mstring_types) \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    210\u001b[0m            \u001b[38;5;28misinstance\u001b[39m(translation_corpus[\u001b[38;5;241m0\u001b[39m], six\u001b[38;5;241m.\u001b[39mstring_types), \\\n\u001b[1;32m    211\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreferences and translation should have format of list(list(str)) \u001b[39m\u001b[38;5;124m'\u001b[39m \\\n\u001b[1;32m    212\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mand list(str), respectively, when tokenized is False.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m references, translation \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mreference_corpus_list), translation_corpus):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tokenized:\n",
      "\u001b[0;31mAssertionError\u001b[0m: references and translation should have format of list(list(str)) and list(str), respectively, when tokenized is False."
     ]
    }
   ],
   "source": [
    "reload(transformer_model)\n",
    "\n",
    "# Let's train\n",
    "trainer = mx.gluon.Trainer(wmt_transformer_model_tl.collect_params(), hparams.optimizer, {'learning_rate': hparams.lr})\n",
    "\n",
    "hparams.epochs = 3\n",
    "\n",
    "loss_function = nlp.loss.MaskedSoftmaxCELoss()\n",
    "loss_function.hybridize(static_alloc=True)\n",
    "\n",
    "wmt_detokenizer = nlp.data.SacreMosesDetokenizer()\n",
    "\n",
    "best_valid_bleu = 0.0\n",
    "\n",
    "wmt2016_train_losses = []\n",
    "wmt2016_valid_losses = []\n",
    "wmt2016_valid_bleus  = []\n",
    "wmt2016_valid_perplexities = []\n",
    "\n",
    "for epoch_id in tqdm(range(hparams.epochs)):\n",
    "\n",
    "    log_loss = 0\n",
    "    log_denom = 0\n",
    "    log_avg_gnorm = 0\n",
    "    log_wc = 0\n",
    "    log_start_time = time.time()\n",
    "\n",
    "    # Iterate through each batch\n",
    "    for batch_id, (src_seq, tgt_seq, src_valid_length, tgt_valid_length) in enumerate(tqdm(wmt2016_train_data_loader)):\n",
    "        \n",
    "        # print(\"XXXXXX TEST XXXXXX:\", src_seq,\n",
    "        #         tgt_seq[:, :-1],\n",
    "        #         src_valid_length,\n",
    "        #         tgt_valid_length - 1)\n",
    "        \n",
    "        src_seq = src_seq.as_in_context(ctx)\n",
    "        tgt_seq = tgt_seq.as_in_context(ctx)\n",
    "        src_valid_length = src_valid_length.as_in_context(ctx)\n",
    "        tgt_valid_length = tgt_valid_length.as_in_context(ctx)\n",
    "        \n",
    "        with mx.autograd.record():\n",
    "            out, _ = wmt_transformer_model_tl(\n",
    "                src_seq,\n",
    "                tgt_seq[:, :-1],\n",
    "                src_valid_length,\n",
    "                tgt_valid_length - 1)\n",
    "\n",
    "            loss = loss_function(out, tgt_seq[:, 1:], tgt_valid_length - 1).mean()\n",
    "            loss = loss * (tgt_seq.shape[1] - 1)\n",
    "            log_loss += loss * tgt_seq.shape[0]\n",
    "            log_denom += (tgt_valid_length - 1).sum()\n",
    "            loss = loss / (tgt_valid_length - 1).mean()\n",
    "            loss.backward()\n",
    "\n",
    "        grads = [p.grad(ctx) for p in wmt_transformer_model_tl.collect_params().values() if p.grad_req != 'null']\n",
    "        gnorm = mx.gluon.utils.clip_global_norm(grads, hparams.clip)\n",
    "        trainer.step(1)\n",
    "        \n",
    "        src_wc = src_valid_length.sum().asscalar()\n",
    "        tgt_wc = (tgt_valid_length - 1).sum().asscalar()\n",
    "        log_loss = log_loss.asscalar()\n",
    "        log_denom = log_denom.asscalar()\n",
    "        log_avg_gnorm += gnorm\n",
    "        log_wc += src_wc + tgt_wc\n",
    "        \n",
    "        wmt2016_train_loss = log_loss / log_denom\n",
    "        wmt2016_train_losses.append(wmt2016_train_loss)\n",
    "        \n",
    "        if (batch_id + 1) % hparams.log_interval == 0:\n",
    "            wps = log_wc / (time.time() - log_start_time)\n",
    "            print(\"[Epoch {} Batch {}/{}] loss={:.4f}, ppl={:.4f}, gnorm={:.4f}, \"\n",
    "                         \"throughput={:.2f}K wps, wc={:.2f}K\"\n",
    "                         .format(epoch_id, batch_id + 1, len(wmt2016_train_data_loader),\n",
    "                                 wmt2016_train_loss,\n",
    "                                 np.exp(log_loss / log_denom),\n",
    "                                 log_avg_gnorm / hparams.log_interval,\n",
    "                                 wps / 1000, log_wc / 1000))\n",
    "            \n",
    "            log_start_time = time.time()\n",
    "            log_loss = 0\n",
    "            log_denom = 0\n",
    "            log_avg_gnorm = 0\n",
    "            log_wc = 0\n",
    "\n",
    "    # Validation step\n",
    "    wmt2016_valid_loss, wmt2016_valid_translation_out = transformer_model.evaluate(\n",
    "        wmt_transformer_model_tl,\n",
    "        wmt2016_val_data_loader,\n",
    "        loss_function,\n",
    "        wmt_translator_tl,\n",
    "        wmt_tgt_vocab,\n",
    "        wmt_detokenizer,\n",
    "        ctx)\n",
    "    \n",
    "    wmt2016_valid_bleu_score, _, _, _, _ = nmt.bleu.compute_bleu(\n",
    "        [wmt2016_val_tgt_sentences],\n",
    "        wmt2016_valid_translation_out,\n",
    "        tokenized=False,\n",
    "        tokenizer=\"13a\",\n",
    "        split_compound_word=False,\n",
    "        bpe=False)\n",
    "\n",
    "    wmt2016_valid_perplexity = np.exp(wmt2016_valid_loss)\n",
    "    wmt_2016_valid_perplexities.append(wmt2016_valid_perplexity)\n",
    "    wmt2016_valid_losses.append(wmt2016_valid_loss)\n",
    "    wmt2016_valid_bleus.append(wmt2016_valid_bleu_score * 100)\n",
    "    \n",
    "    print(\"[Epoch {}] valid Loss={:.4f}, valid ppl={:.4f}, valid bleu={:.2f}\"\n",
    "          .format(epoch_id, wmt2016_valid_loss, wmt2016_valid_perplexity, wmt2016_valid_bleu_score * 100))\n",
    "    \n",
    "    if wmt2016_valid_bleu_score > best_valid_bleu:\n",
    "        best_valid_bleu = valid_bleu_score\n",
    "        print(\"Save best parameters to {}\".format(model_filename_tl))\n",
    "        wmt_transformer_model_tl.save_parameters(model_filename_tl)\n",
    "    \n",
    "    # if epoch_id + 1 >= (hparams.epochs * 2) // 3:\n",
    "    #     new_lr = trainer.learning_rate * hparams.lr_update_factor\n",
    "    #     print(\"Learning rate change to {}\".format(new_lr))\n",
    "    #     trainer.set_learning_rate(new_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e9e58786-6b04-43a1-bbbb-86a7921d75a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mxnet.gluon.data.dataset._LazyTransformDataset at 0x7f29d2cd6e80>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wmt2016_val_tgt_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1f435dde-3d18-4899-831f-32a8732cd82e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mxnet.gluon.data.dataset._LazyTransformDataset at 0x7f2b53b47730>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wmt_test_tgt_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c2a938-e44c-461c-9575-5fa144b4a395",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc902401f25e4d4ab02cd7bb0af737d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/369 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wmt2016_test_loss, wmt2016_test_translation_out = transformer_model.evaluate(\n",
    "    wmt_transformer_model_tl,\n",
    "    wmt2016_test_data_loader,\n",
    "    loss_function,\n",
    "    wmt_translator_tl,\n",
    "    wmt_tgt_vocab,\n",
    "    wmt_detokenizer,\n",
    "    ctx)\n",
    "\n",
    "wmt2016_valid_bleu_score, _, _, _, _ = nmt.bleu.compute_bleu(\n",
    "    [wmt2016_test_tgt_sentences],\n",
    "    wmt2016_test_translation_out,\n",
    "    tokenized=False,\n",
    "    tokenizer=\"13a\",\n",
    "    split_compound_word=False,\n",
    "    bpe=False)\n",
    "\n",
    "print('WMT16 test loss: %.2f; test bleu score: %.2f'\n",
    "      %(wmt_test_loss, wmt_test_bleu_score * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d9038e-9280-4829-968e-1f7b8753a574",
   "metadata": {},
   "source": [
    "## Fine-Tuning (after Transfer Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2a1e2c66-3986-42b9-a1ab-ab5ec72b8e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model built on top of Transfer Learning model\n",
    "# wmt_transformer_model_tl.load_parameters(model_filename_tl)\n",
    "wmt_transformer_model_ft = wmt_transformer_model_tl\n",
    "\n",
    "# Un-freeze weights\n",
    "for param in wmt_transformer_model_ft.collect_params().values(): \n",
    "    if param.name in updated_params:\n",
    "        param.grad_req = 'write'\n",
    "\n",
    "wmt_transformer_model_ft.hybridize()\n",
    "\n",
    "model_filename_ft = \"transformer_en_de_512_ft.params\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b390d4ef-ffc6-4160-81d3-b179cc34a289",
   "metadata": {},
   "outputs": [],
   "source": [
    "wmt_translator_ft = nmt.translation.BeamSearchTranslator(\n",
    "    model=wmt_transformer_model_ft,\n",
    "    beam_size=hparams.beam_size,\n",
    "    scorer=nlp.model.BeamSearchScorer(alpha=hparams.lp_alpha, K=hparams.lp_k),\n",
    "    max_length=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "96e7b1ff-ca5e-44d4-a896-635e8d472932",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22ebe279bf2e45f09af56d18b34cba60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5cdb3343b6a49e6a78c3e16db6679b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/785 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 100/785] loss=4.8289, ppl=125.0683, gnorm=3.4245, throughput=30.42K wps, wc=655.63K\n",
      "[Epoch 0 Batch 200/785] loss=4.1356, ppl=62.5249, gnorm=2.5131, throughput=29.60K wps, wc=582.26K\n",
      "[Epoch 0 Batch 300/785] loss=4.1546, ppl=63.7279, gnorm=2.7742, throughput=30.41K wps, wc=626.88K\n",
      "[Epoch 0 Batch 400/785] loss=4.1127, ppl=61.1099, gnorm=3.4123, throughput=29.14K wps, wc=635.07K\n",
      "[Epoch 0 Batch 500/785] loss=4.0549, ppl=57.6773, gnorm=2.4709, throughput=29.68K wps, wc=662.89K\n",
      "[Epoch 0 Batch 600/785] loss=3.7495, ppl=42.4993, gnorm=2.5455, throughput=30.36K wps, wc=594.40K\n",
      "[Epoch 0 Batch 700/785] loss=3.7386, ppl=42.0393, gnorm=2.0752, throughput=31.23K wps, wc=644.82K\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eca430724d2b439e9c3382c9acfb059c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/785 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 Batch 100/785] loss=3.1350, ppl=22.9897, gnorm=1.9700, throughput=29.35K wps, wc=614.72K\n",
      "[Epoch 1 Batch 200/785] loss=3.2306, ppl=25.2949, gnorm=2.2025, throughput=31.86K wps, wc=661.64K\n",
      "[Epoch 1 Batch 300/785] loss=3.3254, ppl=27.8103, gnorm=5.0830, throughput=31.12K wps, wc=665.94K\n",
      "[Epoch 1 Batch 400/785] loss=3.3146, ppl=27.5108, gnorm=13.7271, throughput=28.07K wps, wc=597.31K\n",
      "[Epoch 1 Batch 500/785] loss=3.3871, ppl=29.5803, gnorm=3.6264, throughput=32.07K wps, wc=676.88K\n",
      "[Epoch 1 Batch 600/785] loss=3.2095, ppl=24.7671, gnorm=2.5420, throughput=28.17K wps, wc=615.45K\n",
      "[Epoch 1 Batch 700/785] loss=3.0889, ppl=21.9533, gnorm=2.3815, throughput=29.50K wps, wc=589.80K\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5841c4be19b340b896e0981f52717159",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/785 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2 Batch 100/785] loss=2.6497, ppl=14.1493, gnorm=2.4708, throughput=28.68K wps, wc=589.19K\n",
      "[Epoch 2 Batch 200/785] loss=2.8581, ppl=17.4278, gnorm=4.8855, throughput=30.29K wps, wc=655.39K\n",
      "[Epoch 2 Batch 300/785] loss=2.7027, ppl=14.9204, gnorm=3.2143, throughput=31.67K wps, wc=594.79K\n",
      "[Epoch 2 Batch 400/785] loss=2.8658, ppl=17.5637, gnorm=6.6410, throughput=31.48K wps, wc=651.27K\n",
      "[Epoch 2 Batch 500/785] loss=2.8147, ppl=16.6881, gnorm=3.2738, throughput=31.63K wps, wc=622.22K\n",
      "[Epoch 2 Batch 600/785] loss=2.9138, ppl=18.4259, gnorm=4.6055, throughput=30.76K wps, wc=648.76K\n",
      "[Epoch 2 Batch 700/785] loss=2.7949, ppl=16.3616, gnorm=3.4296, throughput=31.14K wps, wc=630.86K\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7eeb9edd19742a9ae436d36bbef107e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/785 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     35\u001b[0m grads \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mgrad(ctx) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m wmt_transformer_model_ft\u001b[38;5;241m.\u001b[39mcollect_params()\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgrad_req \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnull\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 36\u001b[0m gnorm \u001b[38;5;241m=\u001b[39m \u001b[43mmx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgluon\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip_global_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m trainer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     38\u001b[0m src_wc \u001b[38;5;241m=\u001b[39m src_valid_length\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39masscalar()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/mxnet/gluon/utils.py:150\u001b[0m, in \u001b[0;36mclip_global_norm\u001b[0;34m(arrays, max_norm, check_isfinite)\u001b[0m\n\u001b[1;32m    148\u001b[0m total_norm \u001b[38;5;241m=\u001b[39m ndarray\u001b[38;5;241m.\u001b[39msqrt(total_norm)\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_isfinite:\n\u001b[0;32m--> 150\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misfinite(\u001b[43mtotal_norm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masscalar\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[1;32m    151\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    152\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnan or inf is detected. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    153\u001b[0m                         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClipping results will be undefined.\u001b[39m\u001b[38;5;124m'\u001b[39m), stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    154\u001b[0m scale \u001b[38;5;241m=\u001b[39m max_norm \u001b[38;5;241m/\u001b[39m (total_norm \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-8\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/mxnet/ndarray/ndarray.py:2590\u001b[0m, in \u001b[0;36mNDArray.asscalar\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2588\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe current array is not a scalar\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2589\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m-> 2590\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   2591\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2592\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masnumpy()[()]\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/mxnet/ndarray/ndarray.py:2568\u001b[0m, in \u001b[0;36mNDArray.asnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2551\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns a ``numpy.ndarray`` object with value copied from this array.\u001b[39;00m\n\u001b[1;32m   2552\u001b[0m \n\u001b[1;32m   2553\u001b[0m \u001b[38;5;124;03mExamples\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2565\u001b[0m \u001b[38;5;124;03m       [1, 1, 1]], dtype=int32)\u001b[39;00m\n\u001b[1;32m   2566\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2567\u001b[0m data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m-> 2568\u001b[0m check_call(\u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMXNDArraySyncCopyToCPU\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2569\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2570\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_as\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_void_p\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2571\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_size_t\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2572\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-37:\n",
      "Process ForkPoolWorker-34:\n",
      "Process ForkPoolWorker-39:\n",
      "Process ForkPoolWorker-38:\n",
      "Process ForkPoolWorker-35:\n",
      "Process ForkPoolWorker-36:\n",
      "Process ForkPoolWorker-40:\n",
      "Process ForkPoolWorker-33:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "reload(transformer_model)\n",
    "\n",
    "trainer = mx.gluon.Trainer(wmt_transformer_model_ft.collect_params(), 'adam', {'learning_rate': hparams.lr})\n",
    "\n",
    "loss_function = nlp.loss.MaskedSoftmaxCELoss()\n",
    "loss_function.hybridize()\n",
    "\n",
    "wmt_detokenizer = nlp.data.SacreMosesDetokenizer()\n",
    "\n",
    "best_valid_bleu = 0.0\n",
    "\n",
    "# Run through each epoch\n",
    "for epoch_id in tqdm(range(hparams.epochs)):\n",
    "    log_avg_loss = 0\n",
    "    log_avg_gnorm = 0\n",
    "    log_wc = 0\n",
    "    log_start_time = time.time()\n",
    "\n",
    "    # Iterate through each batch\n",
    "    for batch_id, (src_seq, tgt_seq, src_valid_length, tgt_valid_length)\\\n",
    "            in enumerate(tqdm(train_data_loader)):\n",
    "\n",
    "        src_seq = src_seq.as_in_context(ctx)\n",
    "        tgt_seq = tgt_seq.as_in_context(ctx)\n",
    "        src_valid_length = src_valid_length.as_in_context(ctx)\n",
    "        tgt_valid_length = tgt_valid_length.as_in_context(ctx)\n",
    "\n",
    "        # Compute gradients and losses\n",
    "        with mx.autograd.record():\n",
    "            out, _ = wmt_transformer_model_ft(src_seq, tgt_seq[:, :-1], src_valid_length, tgt_valid_length - 1)\n",
    "            loss = loss_function(out, tgt_seq[:, 1:], tgt_valid_length - 1).mean()\n",
    "            loss = loss * (tgt_seq.shape[1] - 1) / (tgt_valid_length - 1).mean()\n",
    "            loss.backward()\n",
    "\n",
    "        grads = [p.grad(ctx) for p in wmt_transformer_model_ft.collect_params().values() if p.grad_req != \"null\"]\n",
    "        gnorm = mx.gluon.utils.clip_global_norm(grads, hparams.clip)\n",
    "        trainer.step(1)\n",
    "        src_wc = src_valid_length.sum().asscalar()\n",
    "        tgt_wc = (tgt_valid_length - 1).sum().asscalar()\n",
    "        step_loss = loss.asscalar()\n",
    "        log_avg_loss += step_loss\n",
    "        log_avg_gnorm += gnorm\n",
    "        log_wc += src_wc + tgt_wc\n",
    "        if (batch_id + 1) % hparams.log_interval == 0:\n",
    "            wps = log_wc / (time.time() - log_start_time)\n",
    "            print('[Epoch {} Batch {}/{}] loss={:.4f}, ppl={:.4f}, gnorm={:.4f}, '\n",
    "                         'throughput={:.2f}K wps, wc={:.2f}K'\n",
    "                         .format(epoch_id, batch_id + 1, len(train_data_loader),\n",
    "                                 log_avg_loss / hparams.log_interval,\n",
    "                                 np.exp(log_avg_loss / hparams.log_interval),\n",
    "                                 log_avg_gnorm / hparams.log_interval,\n",
    "                                 wps / 1000, log_wc / 1000))\n",
    "            log_start_time = time.time()\n",
    "            log_avg_loss = 0\n",
    "            log_avg_gnorm = 0\n",
    "            log_wc = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63406a3c-77bb-4a39-a5c5-a181c7f97308",
   "metadata": {},
   "source": [
    "## Fine-Tuning (directly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd2f0bf4-8e7b-4737-b2b0-73e724380c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andreto/.local/lib/python3.8/site-packages/gluonnlp/vocab/vocab.py:590: UserWarning: Detected a corrupted index in the deserialize vocabulary. For versions before GluonNLP v0.7 the index is corrupted by specifying the same token for different special purposes, for example eos_token == padding_token. Deserializing the vocabulary nevertheless.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ctx = mx.gpu()\n",
    "\n",
    "# Build on top of Pre-Trained model\n",
    "wmt_model_name = 'transformer_en_de_512'\n",
    "wmt_transformer_model_ft_direct, _, _ = nlp.model.get_model(\n",
    "    wmt_model_name,\n",
    "    dataset_name='WMT2014',\n",
    "    pretrained=True,\n",
    "    ctx=ctx)\n",
    "\n",
    "# # What if we don't overwrite this?\n",
    "# wmt_transformer_model_ft_direct.tgt_proj = mx.gluon.nn.Dense(units=len(wmt_tgt_vocab), flatten=False, prefix='tgt_proj_')\n",
    "# wmt_transformer_model_ft_direct.tgt_proj.initialize(ctx=ctx)\n",
    "\n",
    "wmt_transformer_model_ft_direct.hybridize()\n",
    "\n",
    "model_filename_ft_direct = \"transformer_en_de_512_ft_direct.params\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c4d2280b-dc1f-4be7-a084-c08bc4f9fd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wmt_translator_ft_direct = nmt.translation.BeamSearchTranslator(\n",
    "    model=wmt_transformer_model_ft_direct,\n",
    "    beam_size=hparams.beam_size,\n",
    "    scorer=nlp.model.BeamSearchScorer(alpha=hparams.lp_alpha, K=hparams.lp_k),\n",
    "    max_length=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "478231d1-a750-4412-aff5-c76d0631868b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22dbcf5cd4624636b493c2ade798f873",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a393dbcf7694b97a6ca1d04c4415d35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/785 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m log_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Iterate through each batch\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_id, (src_seq, tgt_seq, src_valid_length, tgt_valid_length)\\\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(train_data_loader)):\n\u001b[1;32m     23\u001b[0m     src_seq \u001b[38;5;241m=\u001b[39m src_seq\u001b[38;5;241m.\u001b[39mas_in_context(ctx)\n\u001b[1;32m     24\u001b[0m     tgt_seq \u001b[38;5;241m=\u001b[39m tgt_seq\u001b[38;5;241m.\u001b[39mas_in_context(ctx)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tqdm/notebook.py:249\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    248\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m(tqdm_notebook, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[0;32m--> 249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[1;32m    250\u001b[0m         \u001b[38;5;66;03m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[1;32m    251\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m    252\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tqdm/std.py:1182\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1181\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1182\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1185\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/gluonnlp/data/dataloader.py:100\u001b[0m, in \u001b[0;36m_MultiWorkerIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rcvd_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_buffer, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfatal error with _push_next, rcvd_idx missing\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     99\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_buffer\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rcvd_idx)\n\u001b[0;32m--> 100\u001b[0m batch \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mloads(\u001b[43mret\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m ret\u001b[38;5;241m.\u001b[39mget()\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    102\u001b[0m     batch \u001b[38;5;241m=\u001b[39m _as_in_context(batch, context\u001b[38;5;241m.\u001b[39mcpu_pinned())\n",
      "File \u001b[0;32m/usr/lib/python3.8/multiprocessing/pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 765\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    766\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mready():\n\u001b[1;32m    767\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.8/multiprocessing/pool.py:762\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 762\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.8/threading.py:558\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    556\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    557\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 558\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m/usr/lib/python3.8/threading.py:302\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 302\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "reload(transformer_model)\n",
    "\n",
    "trainer = mx.gluon.Trainer(wmt_transformer_model_ft_direct.collect_params(), 'adam', {'learning_rate': hparams.lr})\n",
    "\n",
    "loss_function = nlp.loss.MaskedSoftmaxCELoss()\n",
    "loss_function.hybridize()\n",
    "\n",
    "wmt_detokenizer = nlp.data.SacreMosesDetokenizer()\n",
    "\n",
    "best_valid_bleu = 0.0\n",
    "\n",
    "# Run through each epoch\n",
    "for epoch_id in tqdm(range(hparams.epochs)):\n",
    "    log_avg_loss = 0\n",
    "    log_avg_gnorm = 0\n",
    "    log_wc = 0\n",
    "    log_start_time = time.time()\n",
    "\n",
    "    # Iterate through each batch\n",
    "    for batch_id, (src_seq, tgt_seq, src_valid_length, tgt_valid_length)\\\n",
    "            in enumerate(tqdm(train_data_loader)):\n",
    "\n",
    "        src_seq = src_seq.as_in_context(ctx)\n",
    "        tgt_seq = tgt_seq.as_in_context(ctx)\n",
    "        src_valid_length = src_valid_length.as_in_context(ctx)\n",
    "        tgt_valid_length = tgt_valid_length.as_in_context(ctx)\n",
    "\n",
    "        # Compute gradients and losses\n",
    "        with mx.autograd.record():\n",
    "            out, _ = wmt_transformer_model_ft_direct(src_seq, tgt_seq[:, :-1], src_valid_length, tgt_valid_length - 1)\n",
    "            loss = loss_function(out, tgt_seq[:, 1:], tgt_valid_length - 1).mean()\n",
    "            loss = loss * (tgt_seq.shape[1] - 1) / (tgt_valid_length - 1).mean()\n",
    "            loss.backward()\n",
    "\n",
    "        grads = [p.grad(ctx) for p in wmt_transformer_model_ft_direct.collect_params().values() if p.grad_req != \"null\"]\n",
    "        gnorm = mx.gluon.utils.clip_global_norm(grads, hparams.clip)\n",
    "        trainer.step(1)\n",
    "        src_wc = src_valid_length.sum().asscalar()\n",
    "        tgt_wc = (tgt_valid_length - 1).sum().asscalar()\n",
    "        step_loss = loss.asscalar()\n",
    "        log_avg_loss += step_loss\n",
    "        log_avg_gnorm += gnorm\n",
    "        log_wc += src_wc + tgt_wc\n",
    "        if (batch_id + 1) % hparams.log_interval == 0:\n",
    "            wps = log_wc / (time.time() - log_start_time)\n",
    "            print('[Epoch {} Batch {}/{}] loss={:.4f}, ppl={:.4f}, gnorm={:.4f}, '\n",
    "                         'throughput={:.2f}K wps, wc={:.2f}K'\n",
    "                         .format(epoch_id, batch_id + 1, len(train_data_loader),\n",
    "                                 log_avg_loss / hparams.log_interval,\n",
    "                                 np.exp(log_avg_loss / hparams.log_interval),\n",
    "                                 log_avg_gnorm / hparams.log_interval,\n",
    "                                 wps / 1000, log_wc / 1000))\n",
    "            log_start_time = time.time()\n",
    "            log_avg_loss = 0\n",
    "            log_avg_gnorm = 0\n",
    "            log_wc = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f79a1c-b98f-45da-aa84-3a0ed55f837c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
