{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbbe216b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/andres/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/andres/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/andres/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import mxnet as mx\n",
    "import gluonnlp as nlp\n",
    "\n",
    "# Local Libraries\n",
    "# Load Model locally\n",
    "import textcnn\n",
    "# Load helper functions\n",
    "import utils\n",
    "\n",
    "ctx = mx.gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f17f9f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:18:47] ../src/base.cc:80: cuDNN lib mismatch: linked-against version 8101 != compiled-against version 8100.  Set MXNET_CUDNN_LIB_CHECKING=0 to quiet this warning.\n"
     ]
    }
   ],
   "source": [
    "bert_model, vocab = nlp.model.get_model(\n",
    "    'bert_12_768_12',\n",
    "    dataset_name='book_corpus_wiki_en_uncased',\n",
    "    use_classifier=False,\n",
    "    use_decoder=False,\n",
    "    ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bac926f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nlp.data.BERTTokenizer(vocab, lower=True)\n",
    "transform = nlp.data.BERTSentenceTransform(tokenizer, max_seq_length=512, pair=False, pad=False)\n",
    "sample = transform([\"man\"])\n",
    "words, valid_len, segments = mx.nd.array([sample[0]], ctx=ctx), mx.nd.array([sample[1]], ctx=ctx), mx.nd.array([sample[2]], ctx=ctx)\n",
    "seq_encoding, _ = bert_model(words, segments, valid_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6aa0bda3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n"
     ]
    }
   ],
   "source": [
    "embed_size = seq_encoding.shape[2]\n",
    "print(embed_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1f77292",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Train Set: 25000  (no valid split yet)\n",
      "Size of Test  Set: 25000\n"
     ]
    }
   ],
   "source": [
    "# Loading the dataset\n",
    "full_train_dataset, test_dataset = [nlp.data.IMDB(root='data/imdb', segment=segment)\n",
    "                               for segment in ('train', 'test')]\n",
    "\n",
    "# Dataset Sizes\n",
    "print(\"Size of Train Set:\", len(full_train_dataset), \" (no valid split yet)\")\n",
    "print(\"Size of Test  Set:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c6921ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I went and saw this movie last night after being coaxed to by a few friends of mine. I'll admit that I was reluctant to see it because from what I knew of Ashton Kutcher he was only able to do comedy. I was wrong. Kutcher played the character of Jake Fischer very well, and Kevin Costner played Ben Randall with such professionalism. The sign of a good movie is that it can toy with our emotions. This one did exactly that. The entire theater (which was sold out) was overcome by laughter during the first half of the movie, and were moved to tears during the second half. While exiting the theater I not only saw many women in tears, but many full grown men as well, trying desperately not to let anyone see them crying. This movie was great, and I suggest that you go see it before you judge.\n"
     ]
    }
   ],
   "source": [
    "print(test_dataset[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f26e83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Train Set (Post-filter): 6250  (no valid split yet)\n",
      "Size of Test  Set (Post-filter): 1250\n"
     ]
    }
   ],
   "source": [
    "# Filtering for size\n",
    "# Starting data samples are positive reviews, and ending data samples are negative reviews\n",
    "# Retrieving a balanced sample of both sentiments\n",
    "full_train_dataset = full_train_dataset[:3125] + full_train_dataset[-3125:]\n",
    "test_dataset = test_dataset[:625] + test_dataset[-625:]\n",
    "print(\"Size of Train Set (Post-filter):\", len(full_train_dataset), \" (no valid split yet)\")\n",
    "print(\"Size of Test  Set (Post-filter):\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7791838e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Split 40/10/50\n",
    "# Test dataset at 50% is given by library, validation dataset is 20% of train dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_dataset, val_dataset = train_test_split(full_train_dataset, test_size=int(0.2*len(full_train_dataset)), random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a7b312d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Train Set: 5000\n",
      "Size of Val   Set: 1250\n",
      "Size of Test  Set: 1250\n"
     ]
    }
   ],
   "source": [
    "# Final Dataset Sizes\n",
    "print(\"Size of Train Set:\", len(train_dataset))\n",
    "print(\"Size of Val   Set:\", len(val_dataset))\n",
    "print(\"Size of Test  Set:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75b75eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each input in a batch, needs to have the same length\n",
    "# We need to apply a batchify function to pad with zeros (if necessary)\n",
    "# On the output, we just stack them together to get a batch-size array\n",
    "batchify_fn = nlp.data.batchify.Tuple(nlp.data.batchify.Pad(axis=1, pad_val=0),\n",
    "                                      nlp.data.batchify.Stack())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b1e5898",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 768, 179)\n",
      "--------------------------------------------------------------------------------\n",
      "        Layer (type)                                Output Shape         Param #\n",
      "================================================================================\n",
      "               Input                               (1, 768, 179)               0\n",
      "        Activation-1                     <Symbol conv3_relu_fwd>               0\n",
      "        Activation-2                               (1, 768, 177)               0\n",
      "            Conv1D-3                               (1, 768, 177)         1770240\n",
      "   GlobalMaxPool1D-4                                 (1, 768, 1)               0\n",
      "        Activation-5                     <Symbol conv4_relu_fwd>               0\n",
      "        Activation-6                               (1, 768, 176)               0\n",
      "            Conv1D-7                               (1, 768, 176)         2360064\n",
      "   GlobalMaxPool1D-8                                 (1, 768, 1)               0\n",
      "        Activation-9                     <Symbol conv5_relu_fwd>               0\n",
      "       Activation-10                               (1, 768, 175)               0\n",
      "           Conv1D-11                               (1, 768, 175)         2949888\n",
      "  GlobalMaxPool1D-12                                 (1, 768, 1)               0\n",
      "          Dropout-13                                   (1, 2304)               0\n",
      "            Dense-14                                      (1, 1)            2305\n",
      "          TextCNN-15                                      (1, 1)               0\n",
      "================================================================================\n",
      "Parameters in forward computation graph, duplicate included\n",
      "   Total params: 7082497\n",
      "   Trainable params: 7082497\n",
      "   Non-trainable params: 0\n",
      "Shared params in forward computation graph: 0\n",
      "Unique parameters in model: 7082497\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from importlib import reload\n",
    "reload(textcnn)\n",
    "\n",
    "# CNN with 3 parallel filters\n",
    "text_cnn = textcnn.TextCNN([3, 4, 5], [embed_size, embed_size, embed_size])\n",
    "text_cnn.initialize(mx.init.MSRAPrelu(), ctx)\n",
    "\n",
    "# Formatting single input as expected for the network\n",
    "direct_embeddings_test = transform([test_dataset[0][0]])\n",
    "words = mx.nd.array([direct_embeddings_test[0]], ctx=ctx)\n",
    "valid_len = mx.nd.array([direct_embeddings_test[1]], ctx=ctx)\n",
    "segments = mx.nd.array([direct_embeddings_test[2]], ctx=ctx)\n",
    "seq_output, _ = bert_model(words, segments, valid_len)\n",
    "seq_output_reshaped = seq_output.transpose(axes=[0, 2, 1])\n",
    "\n",
    "print(seq_output_reshaped.shape)\n",
    "\n",
    "text_cnn.summary(seq_output_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f7f2939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[3.7410083]]\n",
      "<NDArray 1x1 @gpu(0)> The review is positive\n"
     ]
    }
   ],
   "source": [
    "review_sentiment = text_cnn(seq_output_reshaped)\n",
    "# We can omit sigmoid processing, outputs of the network\n",
    "# with positive values are positive reviews\n",
    "if review_sentiment >= 0:\n",
    "    print(review_sentiment, \"The review is positive\")\n",
    "else:\n",
    "    print(review_sentiment, \"The review is negative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "022bffad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset processing\n",
    "train_set = []\n",
    "for review, score in train_dataset:\n",
    "    # Processing inputs & applying embeddings\n",
    "    direct_embeddings_test = transform([review])\n",
    "    words = mx.nd.array([direct_embeddings_test[0]], ctx=ctx)\n",
    "    valid_len = mx.nd.array([direct_embeddings_test[1]], ctx=ctx)\n",
    "    segments = mx.nd.array([direct_embeddings_test[2]], ctx=ctx)\n",
    "    seq_output, _ = bert_model(words, segments, valid_len)\n",
    "    embeddings = seq_output.transpose(axes=[0, 2, 1])\n",
    "\n",
    "    # A negative review has a score <= 4\n",
    "    # A positive review has a score >= 7 out of 10\n",
    "    sentiment = int(score > 5)\n",
    "    train_set.append((embeddings, sentiment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1931f375",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_set = []\n",
    "for review, score in val_dataset:\n",
    "    # Processing inputs & applying embeddings\n",
    "    direct_embeddings_test = transform([review])\n",
    "    words = mx.nd.array([direct_embeddings_test[0]], ctx=ctx)\n",
    "    valid_len = mx.nd.array([direct_embeddings_test[1]], ctx=ctx)\n",
    "    segments = mx.nd.array([direct_embeddings_test[2]], ctx=ctx)\n",
    "    seq_output, _ = bert_model(words, segments, valid_len)\n",
    "    embeddings = seq_output.transpose(axes=[0, 2, 1])\n",
    "    \n",
    "    # A negative review has a score <= 4\n",
    "    # A positive review has a score >= 7 out of 10\n",
    "    sentiment = int(score > 5)\n",
    "    val_set.append((embeddings, sentiment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b7d484b",
   "metadata": {},
   "outputs": [
    {
     "ename": "MXNetError",
     "evalue": "Traceback (most recent call last):\n  File \"../src/storage/./pooled_storage_manager.h\", line 161\nMXNetError: cudaMalloc retry failed: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMXNetError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_147458/2059674158.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgluon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_cnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"adam\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"learning_rate\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m training_loss, validation_loss, validation_acc = text_cnn.train(\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/packt/Deep-Learning-with-MXNet-Cookbook/ch06/textcnn.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, loss_fn, trainer, epochs, batch_size, training_set, validation_set, batchify_fn, ctx, model_file_name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;31m# Iterator for Gluon data access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         training_data_iterator = mx.gluon.data.DataLoader(training_set,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                                           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                                                           \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/mxnet/gluon/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, last_batch, batch_sampler, batchify_fn, num_workers, pin_memory, pin_device_id, prefetch, thread_pool, timeout)\u001b[0m\n\u001b[1;32m    608\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_worker_pool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prefetch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefetch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mprefetch\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_workers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m         \u001b[0mnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m         \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/mxnet/ndarray/ndarray.py\u001b[0m in \u001b[0;36mwaitall\u001b[0;34m()\u001b[0m\n\u001b[1;32m    209\u001b[0m        \u001b[0mIf\u001b[0m \u001b[0myour\u001b[0m \u001b[0mmxnet\u001b[0m \u001b[0mcode\u001b[0m \u001b[0mthrows\u001b[0m \u001b[0man\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthen\u001b[0m \u001b[0mwaitall\u001b[0m \u001b[0mcan\u001b[0m \u001b[0mcause\u001b[0m \u001b[0mperformance\u001b[0m \u001b[0mimpact\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \"\"\"\n\u001b[0;32m--> 211\u001b[0;31m     \u001b[0mcheck_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMXNDArrayWaitAll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/mxnet/base.py\u001b[0m in \u001b[0;36mcheck_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    244\u001b[0m     \"\"\"\n\u001b[1;32m    245\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mget_last_ffi_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMXNetError\u001b[0m: Traceback (most recent call last):\n  File \"../src/storage/./pooled_storage_manager.h\", line 161\nMXNetError: cudaMalloc retry failed: out of memory"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "batch_size = 4\n",
    "\n",
    "model_file_name = \"bert_textcnn.params\"\n",
    "\n",
    "loss_fn = mx.gluon.loss.SigmoidBCELoss()\n",
    "trainer = mx.gluon.Trainer(text_cnn.collect_params(), \"adam\", {\"learning_rate\": 0.001})\n",
    "\n",
    "training_loss, validation_loss, validation_acc = text_cnn.train(\n",
    "    loss_fn,\n",
    "    trainer,\n",
    "    epochs,\n",
    "    batch_size,\n",
    "    train_set,\n",
    "    val_set,  \n",
    "    batchify_fn,\n",
    "    ctx,\n",
    "    model_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c736dba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_sentiment = text_cnn(seq_output_reshaped)\n",
    "# We can omit sigmoid processing, outputs of the network\n",
    "# with positive values are positive reviews\n",
    "if review_sentiment >= 0:\n",
    "    print(review_sentiment, \"The review is positive\")\n",
    "else:\n",
    "    print(review_sentiment, \"The review is negative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f9e06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot losses and validation accuracy\n",
    "epochs = 5\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0, epochs), validation_loss[:epochs], label=\"Validation Loss\")\n",
    "plt.plot(np.arange(0, epochs), training_loss[:epochs], label=\"Training Loss\")\n",
    "plt.plot(np.arange(0, epochs), validation_acc[:epochs], label=\"Validation Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.title(\"Losses\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e41875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantitative Evaluation on Test Set\n",
    "test_set = []\n",
    "\n",
    "# Limiting test set size to 5000 samples for memory constrains\n",
    "# It is encouraged to increase this value up to 25000 (actual size of test set)\n",
    "# if your memory permits\n",
    "for review, score in test_dataset:\n",
    "    # Processing inputs & applying embeddings\n",
    "    direct_embeddings_test = transform([review])\n",
    "    words = mx.nd.array([direct_embeddings_test[0]], ctx=ctx)\n",
    "    valid_len = mx.nd.array([direct_embeddings_test[1]], ctx=ctx)\n",
    "    segments = mx.nd.array([direct_embeddings_test[2]], ctx=ctx)\n",
    "    seq_output, _ = bert_model(words, segments, valid_len)\n",
    "    embeddings = seq_output.transpose(axes=[0, 2, 1])\n",
    "    \n",
    "    # A negative review has a score <= 4\n",
    "    # A positive review has a score >= 7 out of 10\n",
    "    sentiment = int(score > 5)\n",
    "    test_set.append((embeddings, sentiment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c67c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing Test dataset for confussion matrix\n",
    "num_test_batches = len(test_set) / batch_size\n",
    "\n",
    "# Iterator on Test dataset\n",
    "test_data_iterator = mx.gluon.data.DataLoader(\n",
    "    test_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    batchify_fn=batchify_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2221290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confussion Matrix Computation (Test set)\n",
    "class_outputs = mx.nd.empty(shape=(len(test_set),))\n",
    "labels = mx.nd.empty(shape=(len(test_set),))\n",
    "\n",
    "test_acc = mx.metric.Accuracy()\n",
    "cumulative_test_loss = 0\n",
    "\n",
    "for index, (data, label) in enumerate(tqdm(test_data_iterator)):\n",
    "    \n",
    "    # Add labels for Confussion Matrix\n",
    "    labels[index * batch_size:(index + 1) * batch_size] = label\n",
    "                \n",
    "    # Processing data from data iterator\n",
    "    data_np = data.as_np_ndarray().as_in_context(ctx)\n",
    "    label_np = label.as_np_ndarray().as_in_context(ctx)\n",
    "\n",
    "    output_np = text_cnn(data_np)\n",
    "    test_loss = loss_fn(output_np, label_np)\n",
    "    current_test_loss = mx.np.mean(test_loss)\n",
    "    cumulative_test_loss += current_test_loss / num_test_batches\n",
    "\n",
    "    # Accuracy\n",
    "    # Comparison between  labels and values output\n",
    "    # Applying threshold for binary classification\n",
    "    # No sigmoid necessary as outputs of the network\n",
    "    # with positive values are positive reviews\n",
    "    class_output = (output_np.as_nd_ndarray() >= 0).astype(\"uint8\").transpose()\n",
    "    class_outputs[index * batch_size:(index + 1) * batch_size] = class_output\n",
    "    test_acc.update(label.as_in_context(ctx), class_output[0])\n",
    "\n",
    "test_acc_value = test_acc.get()[1]\n",
    "print(\"Final Test Accuracy:\", test_acc_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650f2754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the CM\n",
    "confusion_matrix = metrics.confusion_matrix(labels.asnumpy(), class_outputs.asnumpy())\n",
    "disp = metrics.ConfusionMatrixDisplay(confusion_matrix)\n",
    "disp.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
