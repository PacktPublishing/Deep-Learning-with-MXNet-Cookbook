{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training GNMT on IWSLT 2015 Dataset\n",
    "\n",
    "In this notebook, we are going to train Google NMT on IWSLT 2015 English-Vietnamese\n",
    "Dataset. The building process includes four key steps:\n",
    "\n",
    "1. Load and preprocess the dataset\n",
    "\n",
    "2. Create a sampler and `DataLoader`\n",
    "\n",
    "3. Build the actual model\n",
    "\n",
    "4. Write the training algorithm\n",
    "\n",
    "This tutorial will guide you through each of the steps and explain briefly how each works. Please remember to click the download button at the top of the page to download the necessary files to follow this tutorial.\n",
    "\n",
    "## Setup\n",
    "\n",
    "Firstly, we need to setup the environment and import the necessary modules. For this tutorial, a GPU is highly important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "import io\n",
    "import logging\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import gluon\n",
    "import gluonnlp as nlp\n",
    "import nmt\n",
    "nlp.utils.check_version('0.7.0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to specify the hyperparameters for the dataset, the model, and for training and testing time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Logs will be saved to /home/andres/code/public/AMLC19-GluonNLP/03_machine_translation/gnmt_en_vi_u512/gnmt_en_vi_u512.log\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/andres/code/public/AMLC19-GluonNLP/03_machine_translation/gnmt_en_vi_u512'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(100)\n",
    "random.seed(100)\n",
    "mx.random.seed(10000)\n",
    "ctx = mx.gpu(0)\n",
    "\n",
    "# parameters for dataset\n",
    "dataset = 'IWSLT2015'\n",
    "src_lang, tgt_lang = 'en', 'vi'\n",
    "src_max_len, tgt_max_len = 50, 50\n",
    "\n",
    "# parameters for model\n",
    "num_hidden = 512\n",
    "num_layers = 2\n",
    "num_bi_layers = 1\n",
    "dropout = 0.2\n",
    "\n",
    "# parameters for training\n",
    "batch_size, test_batch_size = 128, 32\n",
    "num_buckets = 5\n",
    "epochs = 2\n",
    "clip = 5\n",
    "lr = 0.001\n",
    "lr_update_factor = 0.5\n",
    "log_interval = 10\n",
    "save_dir = 'gnmt_en_vi_u512'\n",
    "\n",
    "#parameters for testing\n",
    "beam_size = 10\n",
    "lp_alpha = 1.0\n",
    "lp_k = 5\n",
    "\n",
    "nmt.utils.logging_config(None, save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and processing the dataset\n",
    "\n",
    "The following shows how to process the dataset and cache the processed dataset\n",
    "for future use. The processing steps include the following:\n",
    "\n",
    "1. Clipping the source and target sequences\n",
    "2. Splitting the string input to a list of tokens\n",
    "3. Mapping the string token onto its integer index in the vocabulary\n",
    "4. Appending the end-of-sentence (EOS) token to source sentence and adding BOS and EOS tokens to the target sentence\n",
    "\n",
    "\n",
    "Firstly, we load and cache the dataset with the two helper functions `cache_dataset` and `load_cached_dataset`. The functions are straightforward and well commented so no further explanation will be given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cache_dataset(dataset, prefix):\n",
    "    \"\"\"Cache the processed npy dataset  the dataset into an npz file\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : gluon.data.SimpleDataset\n",
    "    file_path : str\n",
    "    \"\"\"\n",
    "    if not os.path.exists(nmt._constants.CACHE_PATH):\n",
    "        os.makedirs(nmt._constants.CACHE_PATH)\n",
    "    src_data = np.concatenate([e[0] for e in dataset])\n",
    "    tgt_data = np.concatenate([e[1] for e in dataset])\n",
    "    src_cumlen = np.cumsum([0]+[len(e[0]) for e in dataset])\n",
    "    tgt_cumlen = np.cumsum([0]+[len(e[1]) for e in dataset])\n",
    "    np.savez(os.path.join(nmt._constants.CACHE_PATH, prefix + '.npz'),\n",
    "             src_data=src_data, tgt_data=tgt_data,\n",
    "             src_cumlen=src_cumlen, tgt_cumlen=tgt_cumlen)\n",
    "\n",
    "\n",
    "def load_cached_dataset(prefix):\n",
    "    cached_file_path = os.path.join(nmt._constants.CACHE_PATH, prefix + '.npz')\n",
    "    if os.path.exists(cached_file_path):\n",
    "        print('Load cached data from {}'.format(cached_file_path))\n",
    "        npz_data = np.load(cached_file_path)\n",
    "        src_data, tgt_data, src_cumlen, tgt_cumlen = [npz_data[n] for n in\n",
    "                ['src_data', 'tgt_data', 'src_cumlen', 'tgt_cumlen']]\n",
    "        src_data = np.array([src_data[low:high] for low, high in zip(src_cumlen[:-1], src_cumlen[1:])])\n",
    "        tgt_data = np.array([tgt_data[low:high] for low, high in zip(tgt_cumlen[:-1], tgt_cumlen[1:])])\n",
    "        return gluon.data.ArrayDataset(np.array(src_data), np.array(tgt_data))\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we write the class `TrainValDataTransform` to have easy access to transforming and clipping the source and target sentences. This class also adds the EOS and BOS tokens for cleaner data. Please refer to the comments in the code for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainValDataTransform(object):\n",
    "    \"\"\"Transform the machine translation dataset.\n",
    "\n",
    "    Clip source and the target sentences to the maximum length. For the source sentence, append the\n",
    "    EOS. For the target sentence, append BOS and EOS.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    src_vocab : Vocab\n",
    "    tgt_vocab : Vocab\n",
    "    src_max_len : int\n",
    "    tgt_max_len : int\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, src_vocab, tgt_vocab, src_max_len, tgt_max_len):\n",
    "        # On initialization of the class, we set the class variables\n",
    "        self._src_vocab = src_vocab\n",
    "        self._tgt_vocab = tgt_vocab\n",
    "        self._src_max_len = src_max_len\n",
    "        self._tgt_max_len = tgt_max_len\n",
    "\n",
    "    def __call__(self, src, tgt):\n",
    "        # On actual calling of the class, we perform the clipping then the appending of the EOS and BOS tokens.\n",
    "        if self._src_max_len > 0:\n",
    "            src_sentence = self._src_vocab[src.split()[:self._src_max_len]]\n",
    "        else:\n",
    "            src_sentence = self._src_vocab[src.split()]\n",
    "        if self._tgt_max_len > 0:\n",
    "            tgt_sentence = self._tgt_vocab[tgt.split()[:self._tgt_max_len]]\n",
    "        else:\n",
    "            tgt_sentence = self._tgt_vocab[tgt.split()]\n",
    "        src_sentence.append(self._src_vocab[self._src_vocab.eos_token])\n",
    "        tgt_sentence.insert(0, self._tgt_vocab[self._tgt_vocab.bos_token])\n",
    "        tgt_sentence.append(self._tgt_vocab[self._tgt_vocab.eos_token])\n",
    "        src_npy = np.array(src_sentence, dtype=np.int32)\n",
    "        tgt_npy = np.array(tgt_sentence, dtype=np.int32)\n",
    "        return src_npy, tgt_npy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We leverage the class written above to create a helper function that processes the dataset in very few lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(dataset, src_vocab, tgt_vocab, src_max_len=-1, tgt_max_len=-1):\n",
    "    start = time.time()\n",
    "    dataset_processed = dataset.transform(TrainValDataTransform(src_vocab, tgt_vocab,\n",
    "                                                                src_max_len,\n",
    "                                                                tgt_max_len), lazy=False)\n",
    "    end = time.time()\n",
    "    print('Processing time spent: {}'.format(end - start))\n",
    "    return dataset_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a function `load_translation_data` that combines all the above steps to load the data, check if it's been processed, and if not, process the data. The method returns all of the required data for training, validating, and testing our model. Please refer to the comments in the code for more information on what each piece does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_translation_data(dataset, src_lang='en', tgt_lang='vi'):\n",
    "    \"\"\"Load translation dataset\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : str\n",
    "    src_lang : str, default 'en'\n",
    "    tgt_lang : str, default 'vi'\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data_train_processed : Dataset\n",
    "        The preprocessed training sentence pairs\n",
    "    data_val_processed : Dataset\n",
    "        The preprocessed validation sentence pairs\n",
    "    data_test_processed : Dataset\n",
    "        The preprocessed test sentence pairs\n",
    "    val_tgt_sentences : list\n",
    "        The target sentences in the validation set\n",
    "    test_tgt_sentences : list\n",
    "        The target sentences in the test set\n",
    "    src_vocab : Vocab\n",
    "        Vocabulary of the source language\n",
    "    tgt_vocab : Vocab\n",
    "        Vocabulary of the target language\n",
    "    \"\"\"\n",
    "    common_prefix = 'IWSLT2015_{}_{}_{}_{}'.format(src_lang, tgt_lang,\n",
    "                                                   src_max_len, tgt_max_len)\n",
    "\n",
    "    # Load the three datasets from files\n",
    "    data_train = nlp.data.IWSLT2015('train', src_lang=src_lang, tgt_lang=tgt_lang)\n",
    "    data_val = nlp.data.IWSLT2015('val', src_lang=src_lang, tgt_lang=tgt_lang)\n",
    "    data_test = nlp.data.IWSLT2015('test', src_lang=src_lang, tgt_lang=tgt_lang)\n",
    "    src_vocab, tgt_vocab = data_train.src_vocab, data_train.tgt_vocab\n",
    "    data_train_processed = load_cached_dataset(common_prefix + '_train')\n",
    "\n",
    "    # Check if each dataset has been processed or not, and if not, process and cache them.\n",
    "    if not data_train_processed:\n",
    "        data_train_processed = process_dataset(data_train, src_vocab, tgt_vocab,\n",
    "                                               src_max_len, tgt_max_len)\n",
    "        cache_dataset(data_train_processed, common_prefix + '_train')\n",
    "    data_val_processed = load_cached_dataset(common_prefix + '_val')\n",
    "    if not data_val_processed:\n",
    "        data_val_processed = process_dataset(data_val, src_vocab, tgt_vocab)\n",
    "        cache_dataset(data_val_processed, common_prefix + '_val')\n",
    "    data_test_processed = load_cached_dataset(common_prefix + '_test')\n",
    "    if not data_test_processed:\n",
    "        data_test_processed = process_dataset(data_test, src_vocab, tgt_vocab)\n",
    "        cache_dataset(data_test_processed, common_prefix + '_test')\n",
    "\n",
    "    # Pull out the target sentences for both test and validation\n",
    "    fetch_tgt_sentence = lambda src, tgt: tgt.split()\n",
    "    val_tgt_sentences = list(data_val.transform(fetch_tgt_sentence))\n",
    "    test_tgt_sentences = list(data_test.transform(fetch_tgt_sentence))\n",
    "\n",
    "    # Return all of the necessary pieces we can extract from the data for training our model\n",
    "    return data_train_processed, data_val_processed, data_test_processed, \\\n",
    "           val_tgt_sentences, test_tgt_sentences, src_vocab, tgt_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a last helper function `get_data_lengths` to get the length of the datasets, again, for simplified cleaner code later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_lengths(dataset):\n",
    "    return list(dataset.transform(lambda srg, tgt: (len(srg), len(tgt))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for the last step of processing, we leverage all of our helper functions to keep the code concise and to these 15-20 lines for use in our main. This does all of the aforementioned processing along with storing the necessary information in memory for training our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load cached data from /home/andres/code/public/AMLC19-GluonNLP/03_machine_translation/nmt/cached/IWSLT2015_en_vi_50_50_train.npz\n",
      "Load cached data from /home/andres/code/public/AMLC19-GluonNLP/03_machine_translation/nmt/cached/IWSLT2015_en_vi_50_50_val.npz\n",
      "Load cached data from /home/andres/code/public/AMLC19-GluonNLP/03_machine_translation/nmt/cached/IWSLT2015_en_vi_50_50_test.npz\n"
     ]
    }
   ],
   "source": [
    "data_train, data_val, data_test, val_tgt_sentences, test_tgt_sentences, src_vocab, tgt_vocab\\\n",
    "    = load_translation_data(dataset=dataset, src_lang=src_lang, tgt_lang=tgt_lang)\n",
    "data_train_lengths = get_data_lengths(data_train)\n",
    "data_val_lengths = get_data_lengths(data_val)\n",
    "data_test_lengths = get_data_lengths(data_test)\n",
    "\n",
    "with io.open(os.path.join(save_dir, 'val_gt.txt'), 'w', encoding='utf-8') as of:\n",
    "    for ele in val_tgt_sentences:\n",
    "        of.write(' '.join(ele) + '\\n')\n",
    "\n",
    "with io.open(os.path.join(save_dir, 'test_gt.txt'), 'w', encoding='utf-8') as of:\n",
    "    for ele in test_tgt_sentences:\n",
    "        of.write(' '.join(ele) + '\\n')\n",
    "\n",
    "\n",
    "data_train = data_train.transform(lambda src, tgt: (src, tgt, len(src), len(tgt)), lazy=False)\n",
    "data_val = gluon.data.SimpleDataset([(ele[0], ele[1], len(ele[0]), len(ele[1]), i)\n",
    "                                     for i, ele in enumerate(data_val)])\n",
    "data_test = gluon.data.SimpleDataset([(ele[0], ele[1], len(ele[0]), len(ele[1]), i)\n",
    "                                      for i, ele in enumerate(data_test)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampler and `DataLoader` construction\n",
    "\n",
    "Now, we have obtained and stored all of the relevant data information. The next step\n",
    "is to construct the sampler and `DataLoader`. The first step is to use the `batchify`\n",
    "function, which pads and stacks sequences to form mini-batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batchify_fn = nlp.data.batchify.Tuple(nlp.data.batchify.Pad(pad_val=0),\n",
    "                                            nlp.data.batchify.Pad(pad_val=0),\n",
    "                                            nlp.data.batchify.Stack(dtype='float32'),\n",
    "                                            nlp.data.batchify.Stack(dtype='float32'))\n",
    "test_batchify_fn = nlp.data.batchify.Tuple(nlp.data.batchify.Pad(pad_val=0),\n",
    "                                           nlp.data.batchify.Pad(pad_val=0),\n",
    "                                           nlp.data.batchify.Stack(dtype='float32'),\n",
    "                                           nlp.data.batchify.Stack(dtype='float32'),\n",
    "                                           nlp.data.batchify.Stack())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then construct bucketing samplers, which generate batches by grouping\n",
    "sequences with similar lengths. Here, the bucketing scheme is empirically determined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_scheme = nlp.data.ExpWidthBucket(bucket_len_step=1.2)\n",
    "train_batch_sampler = nlp.data.FixedBucketSampler(lengths=data_train_lengths,\n",
    "                                                  batch_size=batch_size,\n",
    "                                                  num_buckets=num_buckets,\n",
    "                                                  shuffle=True,\n",
    "                                                  bucket_scheme=bucket_scheme)\n",
    "logging.info('Train Batch Sampler:\\n{}'.format(train_batch_sampler.stats()))\n",
    "val_batch_sampler = nlp.data.FixedBucketSampler(lengths=data_val_lengths,\n",
    "                                                batch_size=test_batch_size,\n",
    "                                                num_buckets=num_buckets,\n",
    "                                                shuffle=False)\n",
    "logging.info('Valid Batch Sampler:\\n{}'.format(val_batch_sampler.stats()))\n",
    "test_batch_sampler = nlp.data.FixedBucketSampler(lengths=data_test_lengths,\n",
    "                                                 batch_size=test_batch_size,\n",
    "                                                 num_buckets=num_buckets,\n",
    "                                                 shuffle=False)\n",
    "logging.info('Test Batch Sampler:\\n{}'.format(test_batch_sampler.stats()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the samplers, we can create a `DataLoader`, which is iterable. This simply is a data construct (an iterator) that can feed the model batches at a time. For more information refer to [this](https://mxnet.incubator.apache.org/versions/master/tutorials/gluon/datasets.html) page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = gluon.data.DataLoader(data_train,\n",
    "                                          batch_sampler=train_batch_sampler,\n",
    "                                          batchify_fn=train_batchify_fn,\n",
    "                                          num_workers=4)\n",
    "val_data_loader = gluon.data.DataLoader(data_val,\n",
    "                                        batch_sampler=val_batch_sampler,\n",
    "                                        batchify_fn=test_batchify_fn,\n",
    "                                        num_workers=4)\n",
    "test_data_loader = gluon.data.DataLoader(data_test,\n",
    "                                         batch_sampler=test_batch_sampler,\n",
    "                                         batchify_fn=test_batchify_fn,\n",
    "                                         num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the GNMT model\n",
    "\n",
    "After obtaining the DataLoader, we can finally build the model. The GNMT encoder and decoder\n",
    "can be easily constructed by calling `get_gnmt_encoder_decoder` function. Then, we\n",
    "feed the encoder and decoder to the `NMTModel` to construct the GNMT model.\n",
    "\n",
    "`model.hybridize` allows computation to be done using the symbolic backend. To understand what it means to be \"hybridized,\" please refer to [this](https://mxnet.incubator.apache.org/versions/master/tutorials/gluon/hybrid.html) page on MXNet hybridization and its advantages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10:18:54] ../src/base.cc:80: cuDNN lib mismatch: linked-against version 8101 != compiled-against version 8100.  Set MXNET_CUDNN_LIB_CHECKING=0 to quiet this warning.\n"
     ]
    }
   ],
   "source": [
    "encoder, decoder, one_step_ahead_decoder = nmt.gnmt.get_gnmt_encoder_decoder(\n",
    "    hidden_size=num_hidden, dropout=dropout, num_layers=num_layers,\n",
    "    num_bi_layers=num_bi_layers)\n",
    "model = nlp.model.translation.NMTModel(src_vocab=src_vocab, tgt_vocab=tgt_vocab, encoder=encoder,\n",
    "                                       decoder=decoder, one_step_ahead_decoder=one_step_ahead_decoder,\n",
    "                                       embed_size=num_hidden, prefix='gnmt_')\n",
    "model.initialize(init=mx.init.Uniform(0.1), ctx=ctx)\n",
    "static_alloc = True\n",
    "model.hybridize(static_alloc=static_alloc)\n",
    "logging.info(model)\n",
    "\n",
    "# Due to the paddings, we need to mask out the losses corresponding to padding tokens.\n",
    "loss_function = nlp.loss.MaskedSoftmaxCELoss()\n",
    "loss_function.hybridize(static_alloc=static_alloc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we build the `BeamSearchTranslator` and define a predetermined `BeamSearchScorer` as the heuristical mechanism for the search. For more information on Beam Search and its applications to NLP, check [here](https://en.wikipedia.org/wiki/Beam_search)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = nmt.translation.BeamSearchTranslator(model=model, beam_size=beam_size,\n",
    "                                                  scorer=nlp.model.BeamSearchScorer(alpha=lp_alpha,\n",
    "                                                                                    K=lp_k),\n",
    "                                                  max_length=tgt_max_len + 100)\n",
    "logging.info('Use beam_size={}, alpha={}, K={}'.format(beam_size, lp_alpha, lp_k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the evaluation function as shown in the code block below. The `evaluate` function uses the beam\n",
    "search translator to generate outputs for the validation and testing datasets. Please refer to the comments in the code for more information on what each piece does. In addition, we add the `write_sentences` helper method to easily output the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_loader):\n",
    "    \"\"\"Evaluate given the data loader\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_loader : gluon.data.DataLoader\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    avg_loss : float\n",
    "        Average loss\n",
    "    real_translation_out : list of list of str\n",
    "        The translation output\n",
    "    \"\"\"\n",
    "    translation_out = []\n",
    "    all_inst_ids = []\n",
    "    avg_loss_denom = 0\n",
    "    avg_loss = 0.0\n",
    "\n",
    "    for _, (src_seq, tgt_seq, src_valid_length, tgt_valid_length, inst_ids) \\\n",
    "            in enumerate(data_loader):\n",
    "        src_seq = src_seq.as_in_context(ctx)\n",
    "        tgt_seq = tgt_seq.as_in_context(ctx)\n",
    "        src_valid_length = src_valid_length.as_in_context(ctx)\n",
    "        tgt_valid_length = tgt_valid_length.as_in_context(ctx)\n",
    "\n",
    "        # Calculate Loss\n",
    "        out, _ = model(src_seq, tgt_seq[:, :-1], src_valid_length, tgt_valid_length - 1)\n",
    "        loss = loss_function(out, tgt_seq[:, 1:], tgt_valid_length - 1).mean().asscalar()\n",
    "        all_inst_ids.extend(inst_ids.asnumpy().astype(np.int32).tolist())\n",
    "        avg_loss += loss * (tgt_seq.shape[1] - 1)\n",
    "        avg_loss_denom += (tgt_seq.shape[1] - 1)\n",
    "\n",
    "        # Translate the sequences and score them\n",
    "        samples, _, sample_valid_length =\\\n",
    "            translator.translate(src_seq=src_seq, src_valid_length=src_valid_length)\n",
    "        max_score_sample = samples[:, 0, :].asnumpy()\n",
    "        sample_valid_length = sample_valid_length[:, 0].asnumpy()\n",
    "\n",
    "        # Iterate through the tokens and stitch the tokens together for the sentence\n",
    "        for i in range(max_score_sample.shape[0]):\n",
    "            translation_out.append(\n",
    "                [tgt_vocab.idx_to_token[ele] for ele in\n",
    "                 max_score_sample[i][1:(sample_valid_length[i] - 1)]])\n",
    "\n",
    "    # Calculate the average loss and initialize a None-filled translation list\n",
    "    avg_loss = avg_loss / avg_loss_denom\n",
    "    real_translation_out = [None for _ in range(len(all_inst_ids))]\n",
    "\n",
    "    # Combine all the words/tokens into a sentence for the final translation\n",
    "    for ind, sentence in zip(all_inst_ids, translation_out):\n",
    "        real_translation_out[ind] = sentence\n",
    "\n",
    "    # Return the loss and the translation\n",
    "    return avg_loss, real_translation_out\n",
    "\n",
    "\n",
    "def write_sentences(sentences, file_path):\n",
    "    with io.open(file_path, 'w', encoding='utf-8') as of:\n",
    "        for sent in sentences:\n",
    "            of.write(' '.join(sent) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Before entering the training stage, we need to create a trainer for updating the\n",
    "parameters based on the loss. In the following example, we create a trainer that uses the ADAM\n",
    "optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = gluon.Trainer(model.collect_params(), 'adam', {'learning_rate': lr})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then write the training loop. During the training, we evaluate on the validation and testing datasets every epoch, and record the\n",
    "parameters that give the highest [Bilingual Evaluation Understudy Score (BLEU)](https://www.aclweb.org/anthology/P02-1040.pdf) score on the validation dataset. Before\n",
    "performing forward and backward computation, we first use the `as_in_context` function to copy\n",
    "the mini-batch to the GPU. The statement `with mx.autograd.record()` tells Gluon's\n",
    "backend to compute the gradients for the part inside the block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-04 10:35:13,752 - root - [Epoch 0 Batch 10/1043] loss=2.9757, ppl=19.6036, gnorm=0.3644, throughput=33.03K wps, wc=39.45K\n",
      "2022-06-04 10:35:15,559 - root - [Epoch 0 Batch 20/1043] loss=3.3010, ppl=27.1391, gnorm=0.3432, throughput=40.30K wps, wc=72.79K\n",
      "2022-06-04 10:35:16,849 - root - [Epoch 0 Batch 30/1043] loss=3.1031, ppl=22.2670, gnorm=0.3485, throughput=36.19K wps, wc=46.63K\n",
      "2022-06-04 10:35:18,119 - root - [Epoch 0 Batch 40/1043] loss=3.0771, ppl=21.6944, gnorm=0.3393, throughput=35.99K wps, wc=45.60K\n",
      "2022-06-04 10:35:19,768 - root - [Epoch 0 Batch 50/1043] loss=3.3136, ppl=27.4833, gnorm=0.3132, throughput=38.50K wps, wc=63.44K\n",
      "2022-06-04 10:35:21,391 - root - [Epoch 0 Batch 60/1043] loss=3.1699, ppl=23.8045, gnorm=0.3372, throughput=37.96K wps, wc=61.54K\n",
      "2022-06-04 10:35:23,042 - root - [Epoch 0 Batch 70/1043] loss=3.1656, ppl=23.7019, gnorm=0.3358, throughput=38.88K wps, wc=64.15K\n",
      "2022-06-04 10:35:24,675 - root - [Epoch 0 Batch 80/1043] loss=3.2610, ppl=26.0763, gnorm=0.3301, throughput=37.18K wps, wc=60.64K\n",
      "2022-06-04 10:35:26,159 - root - [Epoch 0 Batch 90/1043] loss=3.1112, ppl=22.4470, gnorm=0.3349, throughput=35.80K wps, wc=53.08K\n",
      "2022-06-04 10:35:27,590 - root - [Epoch 0 Batch 100/1043] loss=3.1679, ppl=23.7580, gnorm=0.3341, throughput=36.55K wps, wc=52.24K\n",
      "2022-06-04 10:35:29,085 - root - [Epoch 0 Batch 110/1043] loss=3.0432, ppl=20.9720, gnorm=0.3495, throughput=37.86K wps, wc=56.55K\n",
      "2022-06-04 10:35:30,711 - root - [Epoch 0 Batch 120/1043] loss=3.1022, ppl=22.2464, gnorm=0.3336, throughput=37.62K wps, wc=61.11K\n",
      "2022-06-04 10:35:31,973 - root - [Epoch 0 Batch 130/1043] loss=3.0337, ppl=20.7747, gnorm=0.3488, throughput=36.06K wps, wc=45.44K\n",
      "2022-06-04 10:35:33,407 - root - [Epoch 0 Batch 140/1043] loss=3.1346, ppl=22.9785, gnorm=0.3465, throughput=36.86K wps, wc=52.80K\n",
      "2022-06-04 10:35:34,822 - root - [Epoch 0 Batch 150/1043] loss=3.2277, ppl=25.2211, gnorm=0.3283, throughput=37.68K wps, wc=53.20K\n",
      "2022-06-04 10:35:36,196 - root - [Epoch 0 Batch 160/1043] loss=3.1483, ppl=23.2973, gnorm=0.3399, throughput=39.30K wps, wc=53.95K\n",
      "2022-06-04 10:35:37,630 - root - [Epoch 0 Batch 170/1043] loss=3.1435, ppl=23.1843, gnorm=0.3546, throughput=40.72K wps, wc=58.34K\n",
      "2022-06-04 10:35:38,908 - root - [Epoch 0 Batch 180/1043] loss=2.9690, ppl=19.4727, gnorm=0.3614, throughput=35.76K wps, wc=45.62K\n",
      "2022-06-04 10:35:40,381 - root - [Epoch 0 Batch 190/1043] loss=3.1482, ppl=23.2933, gnorm=0.3493, throughput=35.98K wps, wc=52.94K\n",
      "2022-06-04 10:35:42,057 - root - [Epoch 0 Batch 200/1043] loss=3.2715, ppl=26.3504, gnorm=0.3235, throughput=39.65K wps, wc=66.42K\n",
      "2022-06-04 10:35:44,129 - root - [Epoch 0 Batch 210/1043] loss=3.4131, ppl=30.3605, gnorm=0.3059, throughput=41.74K wps, wc=86.34K\n",
      "2022-06-04 10:35:46,025 - root - [Epoch 0 Batch 220/1043] loss=3.3226, ppl=27.7322, gnorm=0.3217, throughput=41.81K wps, wc=79.20K\n",
      "2022-06-04 10:35:47,446 - root - [Epoch 0 Batch 230/1043] loss=3.0802, ppl=21.7623, gnorm=0.3562, throughput=36.20K wps, wc=51.40K\n",
      "2022-06-04 10:35:48,598 - root - [Epoch 0 Batch 240/1043] loss=2.9115, ppl=18.3852, gnorm=0.3701, throughput=34.17K wps, wc=39.27K\n",
      "2022-06-04 10:35:50,128 - root - [Epoch 0 Batch 250/1043] loss=3.1760, ppl=23.9510, gnorm=0.3399, throughput=38.60K wps, wc=59.01K\n",
      "2022-06-04 10:35:51,810 - root - [Epoch 0 Batch 260/1043] loss=3.2274, ppl=25.2129, gnorm=0.3405, throughput=40.99K wps, wc=68.93K\n",
      "2022-06-04 10:35:53,246 - root - [Epoch 0 Batch 270/1043] loss=3.1223, ppl=22.6989, gnorm=0.3538, throughput=40.46K wps, wc=57.97K\n",
      "2022-06-04 10:35:54,751 - root - [Epoch 0 Batch 280/1043] loss=3.1198, ppl=22.6415, gnorm=0.3569, throughput=37.36K wps, wc=56.13K\n",
      "2022-06-04 10:35:56,185 - root - [Epoch 0 Batch 290/1043] loss=3.1554, ppl=23.4622, gnorm=0.3558, throughput=39.09K wps, wc=55.95K\n",
      "2022-06-04 10:35:58,147 - root - [Epoch 0 Batch 300/1043] loss=3.2729, ppl=26.3870, gnorm=0.3245, throughput=38.07K wps, wc=74.64K\n",
      "2022-06-04 10:35:59,503 - root - [Epoch 0 Batch 310/1043] loss=3.1066, ppl=22.3446, gnorm=0.3465, throughput=37.30K wps, wc=50.51K\n",
      "2022-06-04 10:36:00,942 - root - [Epoch 0 Batch 320/1043] loss=3.0703, ppl=21.5475, gnorm=0.3528, throughput=38.37K wps, wc=55.16K\n",
      "2022-06-04 10:36:02,771 - root - [Epoch 0 Batch 330/1043] loss=3.1972, ppl=24.4639, gnorm=0.3382, throughput=39.13K wps, wc=71.51K\n",
      "2022-06-04 10:36:04,516 - root - [Epoch 0 Batch 340/1043] loss=3.2462, ppl=25.6933, gnorm=0.4114, throughput=32.56K wps, wc=56.75K\n",
      "2022-06-04 10:36:05,911 - root - [Epoch 0 Batch 350/1043] loss=3.1387, ppl=23.0731, gnorm=0.3708, throughput=37.16K wps, wc=51.70K\n",
      "2022-06-04 10:36:07,231 - root - [Epoch 0 Batch 360/1043] loss=2.9619, ppl=19.3353, gnorm=0.3679, throughput=35.88K wps, wc=47.31K\n",
      "2022-06-04 10:36:08,875 - root - [Epoch 0 Batch 370/1043] loss=3.2459, ppl=25.6843, gnorm=0.3535, throughput=37.98K wps, wc=62.36K\n",
      "2022-06-04 10:36:10,633 - root - [Epoch 0 Batch 380/1043] loss=3.2049, ppl=24.6525, gnorm=0.3409, throughput=42.21K wps, wc=74.09K\n",
      "2022-06-04 10:36:12,116 - root - [Epoch 0 Batch 390/1043] loss=3.1523, ppl=23.3903, gnorm=0.3526, throughput=36.99K wps, wc=54.75K\n",
      "2022-06-04 10:36:13,671 - root - [Epoch 0 Batch 400/1043] loss=3.1960, ppl=24.4344, gnorm=0.3411, throughput=38.46K wps, wc=59.77K\n",
      "2022-06-04 10:36:15,239 - root - [Epoch 0 Batch 410/1043] loss=3.1636, ppl=23.6567, gnorm=0.3412, throughput=37.27K wps, wc=58.37K\n",
      "2022-06-04 10:36:17,040 - root - [Epoch 0 Batch 420/1043] loss=3.2209, ppl=25.0515, gnorm=0.3674, throughput=40.60K wps, wc=73.06K\n",
      "2022-06-04 10:36:18,851 - root - [Epoch 0 Batch 430/1043] loss=3.2014, ppl=24.5665, gnorm=0.3447, throughput=37.18K wps, wc=67.26K\n",
      "2022-06-04 10:36:20,093 - root - [Epoch 0 Batch 440/1043] loss=2.9766, ppl=19.6205, gnorm=0.3647, throughput=34.98K wps, wc=43.38K\n",
      "2022-06-04 10:36:21,617 - root - [Epoch 0 Batch 450/1043] loss=3.0606, ppl=21.3404, gnorm=0.3562, throughput=36.98K wps, wc=56.33K\n",
      "2022-06-04 10:36:23,385 - root - [Epoch 0 Batch 460/1043] loss=3.1767, ppl=23.9676, gnorm=0.3392, throughput=40.47K wps, wc=71.51K\n",
      "2022-06-04 10:36:24,636 - root - [Epoch 0 Batch 470/1043] loss=3.0360, ppl=20.8219, gnorm=0.3625, throughput=36.23K wps, wc=45.24K\n",
      "2022-06-04 10:36:26,361 - root - [Epoch 0 Batch 480/1043] loss=3.2030, ppl=24.6063, gnorm=0.3702, throughput=38.78K wps, wc=66.84K\n",
      "2022-06-04 10:36:28,119 - root - [Epoch 0 Batch 490/1043] loss=3.1969, ppl=24.4563, gnorm=0.3738, throughput=37.26K wps, wc=65.46K\n",
      "2022-06-04 10:36:29,694 - root - [Epoch 0 Batch 500/1043] loss=3.1657, ppl=23.7044, gnorm=0.3520, throughput=37.08K wps, wc=58.31K\n",
      "2022-06-04 10:36:31,180 - root - [Epoch 0 Batch 510/1043] loss=3.0861, ppl=21.8920, gnorm=0.3548, throughput=36.76K wps, wc=54.60K\n",
      "2022-06-04 10:36:32,928 - root - [Epoch 0 Batch 520/1043] loss=3.1563, ppl=23.4832, gnorm=0.3496, throughput=38.43K wps, wc=67.10K\n",
      "2022-06-04 10:36:34,177 - root - [Epoch 0 Batch 530/1043] loss=2.9359, ppl=18.8382, gnorm=0.3765, throughput=34.61K wps, wc=43.21K\n",
      "2022-06-04 10:36:35,761 - root - [Epoch 0 Batch 540/1043] loss=3.1556, ppl=23.4666, gnorm=0.3370, throughput=38.40K wps, wc=60.77K\n",
      "2022-06-04 10:36:37,433 - root - [Epoch 0 Batch 550/1043] loss=3.1506, ppl=23.3495, gnorm=0.3432, throughput=36.13K wps, wc=60.37K\n",
      "2022-06-04 10:36:38,650 - root - [Epoch 0 Batch 560/1043] loss=2.9585, ppl=19.2699, gnorm=0.3722, throughput=37.30K wps, wc=45.32K\n",
      "2022-06-04 10:36:39,956 - root - [Epoch 0 Batch 570/1043] loss=2.9259, ppl=18.6509, gnorm=0.3669, throughput=36.37K wps, wc=47.43K\n",
      "2022-06-04 10:36:41,844 - root - [Epoch 0 Batch 580/1043] loss=3.2336, ppl=25.3703, gnorm=0.3397, throughput=40.68K wps, wc=76.76K\n",
      "2022-06-04 10:36:43,551 - root - [Epoch 0 Batch 590/1043] loss=3.1619, ppl=23.6145, gnorm=0.3332, throughput=37.98K wps, wc=64.72K\n",
      "2022-06-04 10:36:45,185 - root - [Epoch 0 Batch 600/1043] loss=3.0740, ppl=21.6291, gnorm=0.3535, throughput=39.36K wps, wc=64.26K\n",
      "2022-06-04 10:36:46,646 - root - [Epoch 0 Batch 610/1043] loss=3.0932, ppl=22.0470, gnorm=0.3589, throughput=37.53K wps, wc=54.78K\n",
      "2022-06-04 10:36:47,879 - root - [Epoch 0 Batch 620/1043] loss=2.9183, ppl=18.5106, gnorm=0.3752, throughput=36.93K wps, wc=45.42K\n",
      "2022-06-04 10:36:49,023 - root - [Epoch 0 Batch 630/1043] loss=2.7473, ppl=15.6001, gnorm=0.3976, throughput=35.58K wps, wc=40.63K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-04 10:36:50,621 - root - [Epoch 0 Batch 640/1043] loss=3.1062, ppl=22.3356, gnorm=0.3481, throughput=37.11K wps, wc=59.24K\n",
      "2022-06-04 10:36:52,135 - root - [Epoch 0 Batch 650/1043] loss=3.1338, ppl=22.9612, gnorm=0.3556, throughput=38.42K wps, wc=58.11K\n",
      "2022-06-04 10:36:53,489 - root - [Epoch 0 Batch 660/1043] loss=3.0065, ppl=20.2155, gnorm=0.3721, throughput=37.56K wps, wc=50.81K\n",
      "2022-06-04 10:36:54,974 - root - [Epoch 0 Batch 670/1043] loss=2.9858, ppl=19.8021, gnorm=0.3766, throughput=38.98K wps, wc=57.86K\n",
      "2022-06-04 10:36:56,766 - root - [Epoch 0 Batch 680/1043] loss=3.2288, ppl=25.2482, gnorm=0.3278, throughput=41.34K wps, wc=74.05K\n",
      "2022-06-04 10:36:58,561 - root - [Epoch 0 Batch 690/1043] loss=3.1329, ppl=22.9407, gnorm=0.3459, throughput=37.53K wps, wc=67.29K\n",
      "2022-06-04 10:37:00,118 - root - [Epoch 0 Batch 700/1043] loss=3.1009, ppl=22.2190, gnorm=0.3558, throughput=37.30K wps, wc=57.95K\n",
      "2022-06-04 10:37:01,389 - root - [Epoch 0 Batch 710/1043] loss=2.9719, ppl=19.5286, gnorm=0.3700, throughput=36.45K wps, wc=46.26K\n",
      "2022-06-04 10:37:02,582 - root - [Epoch 0 Batch 720/1043] loss=2.8246, ppl=16.8549, gnorm=0.3974, throughput=37.12K wps, wc=44.26K\n",
      "2022-06-04 10:37:04,329 - root - [Epoch 0 Batch 730/1043] loss=3.1417, ppl=23.1433, gnorm=0.3518, throughput=38.64K wps, wc=67.36K\n",
      "2022-06-04 10:37:05,427 - root - [Epoch 0 Batch 740/1043] loss=2.8241, ppl=16.8463, gnorm=0.3897, throughput=34.25K wps, wc=37.58K\n",
      "2022-06-04 10:37:06,966 - root - [Epoch 0 Batch 750/1043] loss=2.9833, ppl=19.7529, gnorm=0.3787, throughput=38.35K wps, wc=58.94K\n",
      "2022-06-04 10:37:08,912 - root - [Epoch 0 Batch 760/1043] loss=3.2404, ppl=25.5435, gnorm=0.3660, throughput=37.14K wps, wc=72.23K\n",
      "2022-06-04 10:37:10,184 - root - [Epoch 0 Batch 770/1043] loss=2.9829, ppl=19.7454, gnorm=0.3704, throughput=35.83K wps, wc=45.53K\n",
      "2022-06-04 10:37:11,503 - root - [Epoch 0 Batch 780/1043] loss=2.9214, ppl=18.5666, gnorm=0.3840, throughput=36.37K wps, wc=47.94K\n",
      "2022-06-04 10:37:12,984 - root - [Epoch 0 Batch 790/1043] loss=3.0550, ppl=21.2208, gnorm=0.3572, throughput=39.75K wps, wc=58.78K\n",
      "2022-06-04 10:37:14,340 - root - [Epoch 0 Batch 800/1043] loss=3.0614, ppl=21.3575, gnorm=0.3632, throughput=37.01K wps, wc=50.07K\n",
      "2022-06-04 10:37:15,856 - root - [Epoch 0 Batch 810/1043] loss=2.9928, ppl=19.9421, gnorm=0.3976, throughput=38.45K wps, wc=58.24K\n",
      "2022-06-04 10:37:17,003 - root - [Epoch 0 Batch 820/1043] loss=2.8827, ppl=17.8617, gnorm=0.3872, throughput=36.44K wps, wc=41.74K\n",
      "2022-06-04 10:37:18,473 - root - [Epoch 0 Batch 830/1043] loss=2.9704, ppl=19.4994, gnorm=0.3615, throughput=37.74K wps, wc=55.44K\n",
      "2022-06-04 10:37:19,770 - root - [Epoch 0 Batch 840/1043] loss=2.9272, ppl=18.6750, gnorm=0.3834, throughput=35.55K wps, wc=46.04K\n",
      "2022-06-04 10:37:21,048 - root - [Epoch 0 Batch 850/1043] loss=2.9068, ppl=18.2987, gnorm=0.3840, throughput=35.21K wps, wc=44.95K\n",
      "2022-06-04 10:37:22,635 - root - [Epoch 0 Batch 860/1043] loss=3.0325, ppl=20.7489, gnorm=0.3536, throughput=39.81K wps, wc=63.15K\n",
      "2022-06-04 10:37:24,012 - root - [Epoch 0 Batch 870/1043] loss=3.0211, ppl=20.5141, gnorm=0.3842, throughput=38.26K wps, wc=52.60K\n",
      "2022-06-04 10:37:25,802 - root - [Epoch 0 Batch 880/1043] loss=3.1569, ppl=23.4973, gnorm=0.3425, throughput=38.04K wps, wc=68.03K\n",
      "2022-06-04 10:37:27,146 - root - [Epoch 0 Batch 890/1043] loss=2.9848, ppl=19.7819, gnorm=0.3616, throughput=35.58K wps, wc=47.77K\n",
      "2022-06-04 10:37:28,734 - root - [Epoch 0 Batch 900/1043] loss=3.0931, ppl=22.0455, gnorm=0.3829, throughput=37.84K wps, wc=60.01K\n",
      "2022-06-04 10:37:30,215 - root - [Epoch 0 Batch 910/1043] loss=2.9497, ppl=19.1008, gnorm=0.3863, throughput=37.71K wps, wc=55.81K\n",
      "2022-06-04 10:37:31,830 - root - [Epoch 0 Batch 920/1043] loss=3.0499, ppl=21.1134, gnorm=0.3669, throughput=38.78K wps, wc=62.61K\n",
      "2022-06-04 10:37:33,555 - root - [Epoch 0 Batch 930/1043] loss=3.1299, ppl=22.8716, gnorm=0.3434, throughput=37.32K wps, wc=64.32K\n",
      "2022-06-04 10:37:35,242 - root - [Epoch 0 Batch 940/1043] loss=3.0641, ppl=21.4161, gnorm=0.3559, throughput=38.28K wps, wc=64.54K\n",
      "2022-06-04 10:37:37,314 - root - [Epoch 0 Batch 950/1043] loss=3.2512, ppl=25.8215, gnorm=0.3230, throughput=42.47K wps, wc=87.95K\n",
      "2022-06-04 10:37:38,676 - root - [Epoch 0 Batch 960/1043] loss=2.9891, ppl=19.8670, gnorm=0.4770, throughput=35.42K wps, wc=48.17K\n",
      "2022-06-04 10:37:39,996 - root - [Epoch 0 Batch 970/1043] loss=2.9601, ppl=19.2991, gnorm=0.3837, throughput=35.05K wps, wc=46.23K\n",
      "2022-06-04 10:37:41,322 - root - [Epoch 0 Batch 980/1043] loss=2.8551, ppl=17.3768, gnorm=0.3790, throughput=35.97K wps, wc=47.61K\n",
      "2022-06-04 10:37:42,809 - root - [Epoch 0 Batch 990/1043] loss=3.0527, ppl=21.1731, gnorm=0.3972, throughput=37.03K wps, wc=55.03K\n",
      "2022-06-04 10:37:44,270 - root - [Epoch 0 Batch 1000/1043] loss=3.0635, ppl=21.4017, gnorm=0.3916, throughput=38.33K wps, wc=55.94K\n",
      "2022-06-04 10:37:46,279 - root - [Epoch 0 Batch 1010/1043] loss=3.1574, ppl=23.5091, gnorm=0.3438, throughput=36.42K wps, wc=73.09K\n",
      "2022-06-04 10:37:47,698 - root - [Epoch 0 Batch 1020/1043] loss=2.8422, ppl=17.1540, gnorm=0.3870, throughput=37.54K wps, wc=53.18K\n",
      "2022-06-04 10:37:48,993 - root - [Epoch 0 Batch 1030/1043] loss=2.8425, ppl=17.1580, gnorm=0.3918, throughput=37.73K wps, wc=48.82K\n",
      "2022-06-04 10:37:50,503 - root - [Epoch 0 Batch 1040/1043] loss=2.9611, ppl=19.3199, gnorm=0.3759, throughput=36.79K wps, wc=55.47K\n",
      "2022-06-04 10:38:07,383 - root - [Epoch 0] valid Loss=2.2573, valid ppl=9.5575, valid bleu=12.87\n",
      "2022-06-04 10:38:21,850 - root - [Epoch 0] test Loss=2.2940, test ppl=9.9142, test bleu=13.61\n",
      "2022-06-04 10:38:21,858 - root - Save best parameters to gnmt_en_vi_u512/valid_best.params\n",
      "2022-06-04 10:38:22,090 - root - Learning rate change to 0.000125\n",
      "2022-06-04 10:38:23,472 - root - [Epoch 1 Batch 10/1043] loss=2.8780, ppl=17.7784, gnorm=0.3610, throughput=34.97K wps, wc=48.30K\n",
      "2022-06-04 10:38:25,097 - root - [Epoch 1 Batch 20/1043] loss=2.9778, ppl=19.6437, gnorm=0.3385, throughput=38.65K wps, wc=62.77K\n",
      "2022-06-04 10:38:26,566 - root - [Epoch 1 Batch 30/1043] loss=2.7893, ppl=16.2690, gnorm=0.3650, throughput=36.40K wps, wc=53.39K\n",
      "2022-06-04 10:38:28,220 - root - [Epoch 1 Batch 40/1043] loss=2.9812, ppl=19.7123, gnorm=0.3364, throughput=37.52K wps, wc=61.98K\n",
      "2022-06-04 10:38:29,775 - root - [Epoch 1 Batch 50/1043] loss=2.9629, ppl=19.3539, gnorm=0.3489, throughput=37.36K wps, wc=58.08K\n",
      "2022-06-04 10:38:31,528 - root - [Epoch 1 Batch 60/1043] loss=3.0279, ppl=20.6538, gnorm=0.3479, throughput=37.70K wps, wc=66.03K\n",
      "2022-06-04 10:38:32,750 - root - [Epoch 1 Batch 70/1043] loss=2.7848, ppl=16.1960, gnorm=0.3646, throughput=36.89K wps, wc=45.03K\n",
      "2022-06-04 10:38:34,201 - root - [Epoch 1 Batch 80/1043] loss=2.7974, ppl=16.4013, gnorm=0.3740, throughput=39.06K wps, wc=56.62K\n",
      "2022-06-04 10:38:35,652 - root - [Epoch 1 Batch 90/1043] loss=2.8702, ppl=17.6402, gnorm=0.3613, throughput=36.18K wps, wc=52.41K\n",
      "2022-06-04 10:38:37,051 - root - [Epoch 1 Batch 100/1043] loss=2.8538, ppl=17.3533, gnorm=0.3631, throughput=36.57K wps, wc=51.10K\n",
      "2022-06-04 10:38:38,531 - root - [Epoch 1 Batch 110/1043] loss=2.9211, ppl=18.5623, gnorm=0.3545, throughput=36.93K wps, wc=54.57K\n",
      "2022-06-04 10:38:40,315 - root - [Epoch 1 Batch 120/1043] loss=3.0319, ppl=20.7376, gnorm=0.3392, throughput=39.91K wps, wc=71.15K\n",
      "2022-06-04 10:38:41,645 - root - [Epoch 1 Batch 130/1043] loss=2.8591, ppl=17.4460, gnorm=0.3721, throughput=36.48K wps, wc=48.45K\n",
      "2022-06-04 10:38:42,931 - root - [Epoch 1 Batch 140/1043] loss=2.7569, ppl=15.7515, gnorm=0.3740, throughput=35.45K wps, wc=45.56K\n",
      "2022-06-04 10:38:44,090 - root - [Epoch 1 Batch 150/1043] loss=2.8070, ppl=16.5600, gnorm=0.3698, throughput=36.38K wps, wc=42.11K\n",
      "2022-06-04 10:38:45,588 - root - [Epoch 1 Batch 160/1043] loss=2.8384, ppl=17.0888, gnorm=0.3770, throughput=37.62K wps, wc=56.27K\n",
      "2022-06-04 10:38:46,798 - root - [Epoch 1 Batch 170/1043] loss=2.6628, ppl=14.3368, gnorm=0.3897, throughput=35.24K wps, wc=42.57K\n",
      "2022-06-04 10:38:48,184 - root - [Epoch 1 Batch 180/1043] loss=2.9162, ppl=18.4715, gnorm=0.3691, throughput=37.82K wps, wc=52.34K\n",
      "2022-06-04 10:38:49,365 - root - [Epoch 1 Batch 190/1043] loss=2.7213, ppl=15.1996, gnorm=0.3886, throughput=36.29K wps, wc=42.82K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-04 10:38:50,587 - root - [Epoch 1 Batch 200/1043] loss=2.7209, ppl=15.1934, gnorm=0.3871, throughput=39.14K wps, wc=47.76K\n",
      "2022-06-04 10:38:52,565 - root - [Epoch 1 Batch 210/1043] loss=3.1255, ppl=22.7707, gnorm=0.3342, throughput=39.87K wps, wc=78.80K\n",
      "2022-06-04 10:38:54,163 - root - [Epoch 1 Batch 220/1043] loss=3.0122, ppl=20.3328, gnorm=0.3535, throughput=38.03K wps, wc=60.71K\n",
      "2022-06-04 10:38:55,780 - root - [Epoch 1 Batch 230/1043] loss=2.8037, ppl=16.5060, gnorm=0.3852, throughput=38.80K wps, wc=62.67K\n",
      "2022-06-04 10:38:57,301 - root - [Epoch 1 Batch 240/1043] loss=2.8096, ppl=16.6029, gnorm=0.3661, throughput=33.75K wps, wc=51.24K\n",
      "2022-06-04 10:38:58,883 - root - [Epoch 1 Batch 250/1043] loss=2.8267, ppl=16.8900, gnorm=0.3677, throughput=37.95K wps, wc=59.99K\n",
      "2022-06-04 10:39:00,395 - root - [Epoch 1 Batch 260/1043] loss=2.8608, ppl=17.4752, gnorm=0.3703, throughput=40.58K wps, wc=61.30K\n",
      "2022-06-04 10:39:02,100 - root - [Epoch 1 Batch 270/1043] loss=3.0069, ppl=20.2256, gnorm=0.3509, throughput=38.71K wps, wc=65.96K\n",
      "2022-06-04 10:39:03,586 - root - [Epoch 1 Batch 280/1043] loss=2.7842, ppl=16.1862, gnorm=0.4307, throughput=35.43K wps, wc=52.61K\n",
      "2022-06-04 10:39:05,343 - root - [Epoch 1 Batch 290/1043] loss=3.0221, ppl=20.5347, gnorm=0.3422, throughput=39.57K wps, wc=69.39K\n",
      "2022-06-04 10:39:06,841 - root - [Epoch 1 Batch 300/1043] loss=2.8600, ppl=17.4610, gnorm=0.3653, throughput=35.77K wps, wc=53.56K\n",
      "2022-06-04 10:39:08,596 - root - [Epoch 1 Batch 310/1043] loss=2.9003, ppl=18.1794, gnorm=0.3512, throughput=35.64K wps, wc=62.48K\n",
      "2022-06-04 10:39:10,057 - root - [Epoch 1 Batch 320/1043] loss=2.7649, ppl=15.8781, gnorm=0.3762, throughput=32.66K wps, wc=47.66K\n",
      "2022-06-04 10:39:11,491 - root - [Epoch 1 Batch 330/1043] loss=2.7063, ppl=14.9732, gnorm=0.3855, throughput=34.42K wps, wc=49.29K\n",
      "2022-06-04 10:39:12,842 - root - [Epoch 1 Batch 340/1043] loss=2.7818, ppl=16.1474, gnorm=0.3791, throughput=31.03K wps, wc=41.87K\n",
      "2022-06-04 10:39:14,377 - root - [Epoch 1 Batch 350/1043] loss=2.8659, ppl=17.5640, gnorm=0.3723, throughput=36.50K wps, wc=55.97K\n",
      "2022-06-04 10:39:15,776 - root - [Epoch 1 Batch 360/1043] loss=2.7993, ppl=16.4337, gnorm=0.3792, throughput=36.65K wps, wc=51.21K\n",
      "2022-06-04 10:39:17,525 - root - [Epoch 1 Batch 370/1043] loss=3.0575, ppl=21.2751, gnorm=0.3488, throughput=38.92K wps, wc=67.99K\n",
      "2022-06-04 10:39:19,224 - root - [Epoch 1 Batch 380/1043] loss=2.9412, ppl=18.9394, gnorm=0.3758, throughput=31.11K wps, wc=52.77K\n",
      "2022-06-04 10:39:20,774 - root - [Epoch 1 Batch 390/1043] loss=3.0092, ppl=20.2715, gnorm=0.3541, throughput=37.23K wps, wc=57.66K\n",
      "2022-06-04 10:39:22,254 - root - [Epoch 1 Batch 400/1043] loss=2.9414, ppl=18.9419, gnorm=0.3577, throughput=38.18K wps, wc=56.42K\n",
      "2022-06-04 10:39:23,796 - root - [Epoch 1 Batch 410/1043] loss=2.9581, ppl=19.2616, gnorm=0.3562, throughput=37.89K wps, wc=58.37K\n",
      "2022-06-04 10:39:25,283 - root - [Epoch 1 Batch 420/1043] loss=2.9188, ppl=18.5195, gnorm=0.3790, throughput=38.15K wps, wc=56.71K\n",
      "2022-06-04 10:39:26,878 - root - [Epoch 1 Batch 430/1043] loss=2.8374, ppl=17.0706, gnorm=0.3716, throughput=36.25K wps, wc=57.78K\n",
      "2022-06-04 10:39:28,512 - root - [Epoch 1 Batch 440/1043] loss=2.9158, ppl=18.4635, gnorm=0.3784, throughput=34.48K wps, wc=56.27K\n",
      "2022-06-04 10:39:29,924 - root - [Epoch 1 Batch 450/1043] loss=2.7497, ppl=15.6374, gnorm=0.3907, throughput=37.82K wps, wc=53.30K\n",
      "2022-06-04 10:39:31,522 - root - [Epoch 1 Batch 460/1043] loss=2.9029, ppl=18.2273, gnorm=0.3647, throughput=37.17K wps, wc=59.35K\n",
      "2022-06-04 10:39:33,116 - root - [Epoch 1 Batch 470/1043] loss=2.9323, ppl=18.7708, gnorm=0.3583, throughput=38.82K wps, wc=61.87K\n",
      "2022-06-04 10:39:35,056 - root - [Epoch 1 Batch 480/1043] loss=3.0363, ppl=20.8286, gnorm=0.3437, throughput=36.68K wps, wc=71.11K\n",
      "2022-06-04 10:39:36,489 - root - [Epoch 1 Batch 490/1043] loss=2.8236, ppl=16.8367, gnorm=0.3655, throughput=37.53K wps, wc=53.70K\n",
      "2022-06-04 10:39:38,559 - root - [Epoch 1 Batch 500/1043] loss=3.0070, ppl=20.2264, gnorm=0.3370, throughput=39.59K wps, wc=81.90K\n",
      "2022-06-04 10:39:40,037 - root - [Epoch 1 Batch 510/1043] loss=2.8559, ppl=17.3901, gnorm=0.4012, throughput=34.79K wps, wc=51.31K\n",
      "2022-06-04 10:39:41,527 - root - [Epoch 1 Batch 520/1043] loss=2.8958, ppl=18.0980, gnorm=0.3664, throughput=35.55K wps, wc=52.81K\n",
      "2022-06-04 10:39:43,167 - root - [Epoch 1 Batch 530/1043] loss=2.8626, ppl=17.5075, gnorm=0.3732, throughput=34.91K wps, wc=57.18K\n",
      "2022-06-04 10:39:44,623 - root - [Epoch 1 Batch 540/1043] loss=2.8609, ppl=17.4774, gnorm=0.3807, throughput=37.73K wps, wc=54.85K\n",
      "2022-06-04 10:39:45,951 - root - [Epoch 1 Batch 550/1043] loss=2.7828, ppl=16.1649, gnorm=0.3915, throughput=37.69K wps, wc=49.92K\n",
      "2022-06-04 10:39:47,697 - root - [Epoch 1 Batch 560/1043] loss=2.9358, ppl=18.8364, gnorm=0.3645, throughput=37.52K wps, wc=65.42K\n",
      "2022-06-04 10:39:49,290 - root - [Epoch 1 Batch 570/1043] loss=2.8318, ppl=16.9754, gnorm=0.3722, throughput=37.49K wps, wc=59.63K\n",
      "2022-06-04 10:39:51,062 - root - [Epoch 1 Batch 580/1043] loss=2.9983, ppl=20.0512, gnorm=0.3581, throughput=35.46K wps, wc=62.77K\n",
      "2022-06-04 10:39:52,782 - root - [Epoch 1 Batch 590/1043] loss=2.8570, ppl=17.4084, gnorm=0.3685, throughput=36.40K wps, wc=62.55K\n",
      "2022-06-04 10:39:54,532 - root - [Epoch 1 Batch 600/1043] loss=3.0345, ppl=20.7906, gnorm=0.3402, throughput=36.73K wps, wc=64.24K\n",
      "2022-06-04 10:39:55,982 - root - [Epoch 1 Batch 610/1043] loss=2.9313, ppl=18.7516, gnorm=0.3623, throughput=36.08K wps, wc=52.25K\n",
      "2022-06-04 10:39:57,208 - root - [Epoch 1 Batch 620/1043] loss=2.8168, ppl=16.7225, gnorm=0.3776, throughput=36.29K wps, wc=44.45K\n",
      "2022-06-04 10:39:58,906 - root - [Epoch 1 Batch 630/1043] loss=2.9690, ppl=19.4725, gnorm=0.3698, throughput=37.92K wps, wc=64.33K\n",
      "2022-06-04 10:40:00,622 - root - [Epoch 1 Batch 640/1043] loss=2.9938, ppl=19.9624, gnorm=0.3454, throughput=36.41K wps, wc=62.45K\n",
      "2022-06-04 10:40:02,474 - root - [Epoch 1 Batch 650/1043] loss=2.9745, ppl=19.5804, gnorm=0.3483, throughput=40.47K wps, wc=74.92K\n",
      "2022-06-04 10:40:04,067 - root - [Epoch 1 Batch 660/1043] loss=2.8548, ppl=17.3701, gnorm=0.3935, throughput=38.24K wps, wc=60.83K\n",
      "2022-06-04 10:40:05,431 - root - [Epoch 1 Batch 670/1043] loss=2.7576, ppl=15.7619, gnorm=0.3786, throughput=36.29K wps, wc=49.45K\n",
      "2022-06-04 10:40:06,981 - root - [Epoch 1 Batch 680/1043] loss=2.9226, ppl=18.5891, gnorm=0.3864, throughput=37.88K wps, wc=58.65K\n",
      "2022-06-04 10:40:08,468 - root - [Epoch 1 Batch 690/1043] loss=2.7645, ppl=15.8717, gnorm=0.3925, throughput=36.21K wps, wc=53.79K\n",
      "2022-06-04 10:40:09,685 - root - [Epoch 1 Batch 700/1043] loss=2.7063, ppl=14.9740, gnorm=0.3977, throughput=36.11K wps, wc=43.93K\n",
      "2022-06-04 10:40:11,200 - root - [Epoch 1 Batch 710/1043] loss=2.7909, ppl=16.2959, gnorm=0.3742, throughput=36.59K wps, wc=55.36K\n",
      "2022-06-04 10:40:13,125 - root - [Epoch 1 Batch 720/1043] loss=3.0232, ppl=20.5572, gnorm=0.3382, throughput=37.92K wps, wc=72.95K\n",
      "2022-06-04 10:40:14,775 - root - [Epoch 1 Batch 730/1043] loss=2.9417, ppl=18.9479, gnorm=0.3536, throughput=41.47K wps, wc=68.33K\n",
      "2022-06-04 10:40:16,575 - root - [Epoch 1 Batch 740/1043] loss=3.0309, ppl=20.7164, gnorm=0.3484, throughput=42.33K wps, wc=76.07K\n",
      "2022-06-04 10:40:18,029 - root - [Epoch 1 Batch 750/1043] loss=2.8639, ppl=17.5294, gnorm=0.3716, throughput=40.33K wps, wc=58.58K\n",
      "2022-06-04 10:40:19,440 - root - [Epoch 1 Batch 760/1043] loss=2.8161, ppl=16.7118, gnorm=0.3767, throughput=35.91K wps, wc=50.62K\n",
      "2022-06-04 10:40:21,004 - root - [Epoch 1 Batch 770/1043] loss=2.8558, ppl=17.3884, gnorm=0.3753, throughput=36.22K wps, wc=56.55K\n",
      "2022-06-04 10:40:22,611 - root - [Epoch 1 Batch 780/1043] loss=2.8413, ppl=17.1380, gnorm=0.3766, throughput=36.00K wps, wc=57.79K\n",
      "2022-06-04 10:40:23,861 - root - [Epoch 1 Batch 790/1043] loss=2.6729, ppl=14.4822, gnorm=0.3933, throughput=35.25K wps, wc=44.04K\n",
      "2022-06-04 10:40:25,387 - root - [Epoch 1 Batch 800/1043] loss=2.8955, ppl=18.0931, gnorm=0.3740, throughput=36.93K wps, wc=56.28K\n",
      "2022-06-04 10:40:26,959 - root - [Epoch 1 Batch 810/1043] loss=2.9004, ppl=18.1814, gnorm=0.3661, throughput=37.43K wps, wc=58.79K\n",
      "2022-06-04 10:40:28,838 - root - [Epoch 1 Batch 820/1043] loss=3.0081, ppl=20.2496, gnorm=0.3544, throughput=37.84K wps, wc=71.02K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-04 10:40:30,548 - root - [Epoch 1 Batch 830/1043] loss=2.9083, ppl=18.3264, gnorm=0.3627, throughput=36.92K wps, wc=63.06K\n",
      "2022-06-04 10:40:32,245 - root - [Epoch 1 Batch 840/1043] loss=2.9192, ppl=18.5271, gnorm=0.4203, throughput=30.03K wps, wc=50.93K\n",
      "2022-06-04 10:40:33,533 - root - [Epoch 1 Batch 850/1043] loss=2.7070, ppl=14.9841, gnorm=0.3833, throughput=34.27K wps, wc=44.02K\n",
      "2022-06-04 10:40:35,280 - root - [Epoch 1 Batch 860/1043] loss=2.9784, ppl=19.6568, gnorm=0.3637, throughput=42.00K wps, wc=73.29K\n",
      "2022-06-04 10:40:36,715 - root - [Epoch 1 Batch 870/1043] loss=2.7533, ppl=15.6949, gnorm=0.3989, throughput=35.39K wps, wc=50.72K\n",
      "2022-06-04 10:40:38,245 - root - [Epoch 1 Batch 880/1043] loss=2.7501, ppl=15.6437, gnorm=0.3766, throughput=37.01K wps, wc=56.58K\n",
      "2022-06-04 10:40:39,429 - root - [Epoch 1 Batch 890/1043] loss=2.6185, ppl=13.7157, gnorm=0.4051, throughput=35.01K wps, wc=41.41K\n",
      "2022-06-04 10:40:41,055 - root - [Epoch 1 Batch 900/1043] loss=2.9345, ppl=18.8118, gnorm=0.3654, throughput=36.61K wps, wc=59.47K\n",
      "2022-06-04 10:40:42,897 - root - [Epoch 1 Batch 910/1043] loss=3.0198, ppl=20.4869, gnorm=0.3495, throughput=36.43K wps, wc=67.01K\n",
      "2022-06-04 10:40:44,426 - root - [Epoch 1 Batch 920/1043] loss=2.9182, ppl=18.5076, gnorm=0.3840, throughput=35.94K wps, wc=54.88K\n",
      "2022-06-04 10:40:46,215 - root - [Epoch 1 Batch 930/1043] loss=3.0161, ppl=20.4115, gnorm=0.3414, throughput=38.42K wps, wc=68.66K\n",
      "2022-06-04 10:40:47,816 - root - [Epoch 1 Batch 940/1043] loss=2.8240, ppl=16.8448, gnorm=0.3824, throughput=34.87K wps, wc=55.74K\n",
      "2022-06-04 10:40:49,296 - root - [Epoch 1 Batch 950/1043] loss=2.8552, ppl=17.3775, gnorm=0.3805, throughput=35.11K wps, wc=51.87K\n",
      "2022-06-04 10:40:50,622 - root - [Epoch 1 Batch 960/1043] loss=2.8042, ppl=16.5146, gnorm=0.3919, throughput=35.49K wps, wc=47.02K\n",
      "2022-06-04 10:40:52,138 - root - [Epoch 1 Batch 970/1043] loss=2.8070, ppl=16.5603, gnorm=0.3885, throughput=36.76K wps, wc=55.68K\n",
      "2022-06-04 10:40:53,779 - root - [Epoch 1 Batch 980/1043] loss=2.8598, ppl=17.4579, gnorm=0.3786, throughput=35.48K wps, wc=58.19K\n",
      "2022-06-04 10:40:55,660 - root - [Epoch 1 Batch 990/1043] loss=2.9831, ppl=19.7493, gnorm=0.3581, throughput=37.49K wps, wc=70.45K\n",
      "2022-06-04 10:40:57,518 - root - [Epoch 1 Batch 1000/1043] loss=2.9255, ppl=18.6431, gnorm=0.3721, throughput=38.73K wps, wc=71.89K\n",
      "2022-06-04 10:40:59,190 - root - [Epoch 1 Batch 1010/1043] loss=2.8503, ppl=17.2934, gnorm=0.3666, throughput=35.46K wps, wc=59.20K\n",
      "2022-06-04 10:41:00,928 - root - [Epoch 1 Batch 1020/1043] loss=2.9530, ppl=19.1630, gnorm=0.3610, throughput=37.30K wps, wc=64.74K\n",
      "2022-06-04 10:41:02,437 - root - [Epoch 1 Batch 1030/1043] loss=2.7237, ppl=15.2366, gnorm=0.3790, throughput=34.09K wps, wc=51.42K\n",
      "2022-06-04 10:41:03,542 - root - [Epoch 1 Batch 1040/1043] loss=2.7194, ppl=15.1710, gnorm=0.3996, throughput=36.98K wps, wc=40.81K\n",
      "2022-06-04 10:41:20,447 - root - [Epoch 1] valid Loss=2.1976, valid ppl=9.0033, valid bleu=14.33\n",
      "2022-06-04 10:41:34,945 - root - [Epoch 1] test Loss=2.2258, test ppl=9.2605, test bleu=15.33\n",
      "2022-06-04 10:41:34,953 - root - Save best parameters to gnmt_en_vi_u512/valid_best.params\n",
      "2022-06-04 10:41:35,247 - root - Learning rate change to 6.25e-05\n"
     ]
    }
   ],
   "source": [
    "best_valid_bleu = 0.0\n",
    "\n",
    "# Run through each epoch\n",
    "for epoch_id in range(epochs):\n",
    "    log_avg_loss = 0\n",
    "    log_avg_gnorm = 0\n",
    "    log_wc = 0\n",
    "    log_start_time = time.time()\n",
    "\n",
    "    # Iterate through each batch\n",
    "    for batch_id, (src_seq, tgt_seq, src_valid_length, tgt_valid_length)\\\n",
    "            in enumerate(train_data_loader):\n",
    "\n",
    "        src_seq = src_seq.as_in_context(ctx)\n",
    "        tgt_seq = tgt_seq.as_in_context(ctx)\n",
    "        src_valid_length = src_valid_length.as_in_context(ctx)\n",
    "        tgt_valid_length = tgt_valid_length.as_in_context(ctx)\n",
    "\n",
    "        # Compute gradients and losses\n",
    "        with mx.autograd.record():\n",
    "            out, _ = model(src_seq, tgt_seq[:, :-1], src_valid_length, tgt_valid_length - 1)\n",
    "            loss = loss_function(out, tgt_seq[:, 1:], tgt_valid_length - 1).mean()\n",
    "            loss = loss * (tgt_seq.shape[1] - 1) / (tgt_valid_length - 1).mean()\n",
    "            loss.backward()\n",
    "\n",
    "        grads = [p.grad(ctx) for p in model.collect_params().values()]\n",
    "        gnorm = gluon.utils.clip_global_norm(grads, clip)\n",
    "        trainer.step(1)\n",
    "        src_wc = src_valid_length.sum().asscalar()\n",
    "        tgt_wc = (tgt_valid_length - 1).sum().asscalar()\n",
    "        step_loss = loss.asscalar()\n",
    "        log_avg_loss += step_loss\n",
    "        log_avg_gnorm += gnorm\n",
    "        log_wc += src_wc + tgt_wc\n",
    "        if (batch_id + 1) % log_interval == 0:\n",
    "            wps = log_wc / (time.time() - log_start_time)\n",
    "            logging.info('[Epoch {} Batch {}/{}] loss={:.4f}, ppl={:.4f}, gnorm={:.4f}, '\n",
    "                         'throughput={:.2f}K wps, wc={:.2f}K'\n",
    "                         .format(epoch_id, batch_id + 1, len(train_data_loader),\n",
    "                                 log_avg_loss / log_interval,\n",
    "                                 np.exp(log_avg_loss / log_interval),\n",
    "                                 log_avg_gnorm / log_interval,\n",
    "                                 wps / 1000, log_wc / 1000))\n",
    "            log_start_time = time.time()\n",
    "            log_avg_loss = 0\n",
    "            log_avg_gnorm = 0\n",
    "            log_wc = 0\n",
    "\n",
    "    # Evaluate the losses on validation and test datasets and find the corresponding BLEU score and log it\n",
    "    valid_loss, valid_translation_out = evaluate(val_data_loader)\n",
    "    valid_bleu_score, _, _, _, _ = nmt.bleu.compute_bleu([val_tgt_sentences], valid_translation_out)\n",
    "    logging.info('[Epoch {}] valid Loss={:.4f}, valid ppl={:.4f}, valid bleu={:.2f}'\n",
    "                 .format(epoch_id, valid_loss, np.exp(valid_loss), valid_bleu_score * 100))\n",
    "    test_loss, test_translation_out = evaluate(test_data_loader)\n",
    "    test_bleu_score, _, _, _, _ = nmt.bleu.compute_bleu([test_tgt_sentences], test_translation_out)\n",
    "    logging.info('[Epoch {}] test Loss={:.4f}, test ppl={:.4f}, test bleu={:.2f}'\n",
    "                 .format(epoch_id, test_loss, np.exp(test_loss), test_bleu_score * 100))\n",
    "\n",
    "    # Output the sentences we predicted on the validation and test datasets             \n",
    "    write_sentences(valid_translation_out,\n",
    "                    os.path.join(save_dir, 'epoch{:d}_valid_out.txt').format(epoch_id))\n",
    "    write_sentences(test_translation_out,\n",
    "                    os.path.join(save_dir, 'epoch{:d}_test_out.txt').format(epoch_id))\n",
    "\n",
    "    # Save the model if the BLEU score is better than the previous best\n",
    "    if valid_bleu_score > best_valid_bleu:\n",
    "        best_valid_bleu = valid_bleu_score\n",
    "        save_path = os.path.join(save_dir, 'valid_best.params')\n",
    "        logging.info('Save best parameters to {}'.format(save_path))\n",
    "        model.save_parameters(save_path)\n",
    "\n",
    "    # Update the learning rate based on the number of epochs that have passed\n",
    "    if epoch_id + 1 >= (epochs * 2) // 3:\n",
    "        new_lr = trainer.learning_rate * lr_update_factor\n",
    "        logging.info('Learning rate change to {}'.format(new_lr))\n",
    "        trainer.set_learning_rate(new_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this notebook, we have shown how to train a GNMT model on the IWSLT 2015 English-Vietnamese dataset using the Gluon NLP toolkit.\n",
    "The complete training script can be found [here](https://github.com/dmlc/gluon-nlp/blob/master/scripts/machine_translation/train_gnmt.py).\n",
    "The code sequence to reproduce the results can be seen on the [machine translation page](http://gluon-nlp.mxnet.io/model_zoo/machine_translation/index.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
