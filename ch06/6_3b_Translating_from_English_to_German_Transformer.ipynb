{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbbe216b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/andreto/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/andreto/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/andreto/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import mxnet as mx\n",
    "import gluonnlp as nlp\n",
    "\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import sacremoses\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "# Local Libraries\n",
    "import nmt\n",
    "import dataprocessor\n",
    "import utils\n",
    "import nmt.transformer_hparams\n",
    "\n",
    "# Seeds for reproducibility\n",
    "np.random.seed(100)\n",
    "random.seed(100)\n",
    "mx.random.seed(10000)\n",
    "\n",
    "# CPU setup\n",
    "# ctx = mx.cpu()\n",
    "# Single GPU setup\n",
    "ctx = mx.gpu(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bac926f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WMT2016 Dataset (Train and Evaluation)\n",
    "\n",
    "# Dataset Parameters\n",
    "src_lang, tgt_lang = 'en', 'de'\n",
    "\n",
    "wmt_train_text_bpe = nlp.data.WMT2016BPE(\"train\", # BPE: cheapest --> cheap@@, est\n",
    "                                         src_lang=src_lang,\n",
    "                                         tgt_lang=tgt_lang)\n",
    "\n",
    "wmt_train_text     = nlp.data.WMT2016(\"train\",\n",
    "                                      src_lang=src_lang,\n",
    "                                      tgt_lang=tgt_lang)\n",
    "\n",
    "wmt_test_text_bpe  = nlp.data.WMT2016BPE(\"newstest2016\", # BPE: cheapest --> cheap@@, est\n",
    "                                         src_lang=src_lang,\n",
    "                                         tgt_lang=tgt_lang)\n",
    "\n",
    "wmt_test_text      = nlp.data.WMT2016(\"newstest2016\",\n",
    "                                     src_lang=src_lang,\n",
    "                                     tgt_lang=tgt_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6aa0bda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing datasets\n",
    "# Filtering training data to a maximum number of samples,\n",
    "# so that training can be handled in a reasonable time (~8 hrs)\n",
    "# in single GPU setups\n",
    "max_samples = int(1e4)\n",
    "wmt_train_text_bpe = mx.gluon.data.SimpleDataset([wmt_train_text_bpe[i] for i in range(max_samples)])\n",
    "wmt_train_text     = mx.gluon.data.SimpleDataset([wmt_train_text[i] for i in range(max_samples)])\n",
    "wmt_test_text_bpe  = mx.gluon.data.SimpleDataset(wmt_test_text_bpe)\n",
    "wmt_test_text      = mx.gluon.data.SimpleDataset(wmt_test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1f77292",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "By the end of the day, there would be one more death: Lamb took his own life as police closed in on him.\n",
      "Bis zum Ende des Tages gab es einen weiteren Tod: Lamm nahm sich das Leben, als die Polizei ihn einkesselte.\n"
     ]
    }
   ],
   "source": [
    "# Dataset example (human-readable): English and German\n",
    "print(wmt_test_text[16][0])\n",
    "print(wmt_test_text[16][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "237ae7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample target sentence:\n",
      "Bis zum Ende des Tages gab es einen weiteren Tod: Lamm nahm sich das Leben, als die Polizei ihn einkesselte.\n"
     ]
    }
   ],
   "source": [
    "# Retrieve (split) translated sequences (target)\n",
    "wmt_train_tgt_sentences = wmt_train_text.transform(lambda src, tgt: tgt)\n",
    "wmt_test_tgt_sentences  = wmt_test_text.transform(lambda src, tgt: tgt)\n",
    "print(\"Sample target sentence:\")\n",
    "print(wmt_test_tgt_sentences[16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af12498b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "transformer_model, wmt_src_vocab, wmt_tgt_vocab = nlp.model.get_model(\n",
    "    \"transformer_en_de_512\",\n",
    "    dataset_name=\"WMT2014\",\n",
    "    pretrained=True,\n",
    "    # pretrained=False,\n",
    "    ctx=ctx)\n",
    "\n",
    "transformer_model.initialize(init=mx.init.Uniform(0.1), ctx=ctx)\n",
    "static_alloc = True\n",
    "transformer_model.hybridize(static_alloc=static_alloc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd74a96a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2105 28768 16772 23915 28768 15253    24 28798 31241 12938 24036 23280\n",
      " 15283   576  5994 12819 29096 20120 24350 22332 12302 24827 14456 20608\n",
      " 24023 20084    58     3]\n",
      "[    2  1914 31623  3287 15561  9451 18640 17378 16427 30867  9672   576\n",
      "  5994 22652 23470 27121 15222  6064    24 11711 15705  7698 20480 16414\n",
      " 21512 26873 28556    58     3]\n"
     ]
    }
   ],
   "source": [
    "# Dataset processing: clipping, tokenizing, indexing and adding of EOS (src/tgt) / BOS (tgt)\n",
    "wmt_transform_fn = dataprocessor.TrainValDataTransform(wmt_src_vocab, wmt_tgt_vocab)\n",
    "\n",
    "wmt_train_processed = wmt_train_text_bpe.transform(wmt_transform_fn, lazy=False)\n",
    "wmt_test_processed  = wmt_test_text_bpe.transform(wmt_transform_fn, lazy=False)\n",
    "\n",
    "wmt_train_text_with_len = wmt_train_processed.transform(nmt.utils.get_length_index_fn(), lazy=False)\n",
    "wmt_test_text_with_len  = wmt_test_processed.transform(nmt.utils.get_length_index_fn(), lazy=False)\n",
    "\n",
    "print(wmt_test_text_with_len[16][0])\n",
    "print(wmt_test_text_with_len[16][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b817d057",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andreto/.local/lib/python3.7/site-packages/gluonnlp/data/batchify/batchify.py:235: UserWarning: Padding value is not given and will be set automatically to 0 in data.batchify.Pad(). Please check whether this is intended (e.g. value of padding index in the vocabulary).\n",
      "  'Padding value is not given and will be set automatically to 0 '\n"
     ]
    }
   ],
   "source": [
    "# Batcher\n",
    "wmt_batchify_fn = nlp.data.batchify.Tuple(\n",
    "    nlp.data.batchify.Pad(),                   # Source Token IDs\n",
    "    nlp.data.batchify.Pad(),                   # Target Token IDs\n",
    "    nlp.data.batchify.Stack(dtype='float32'),  # Source Sequence Length\n",
    "    nlp.data.batchify.Stack(dtype='float32'),  # Target Sequence Length\n",
    "    nlp.data.batchify.Stack())                 # Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d10c420b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "hparams = nmt.transformer_hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f3c4dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FixedBucketSampler:\n",
      "  sample_num=10000, batch_num=168\n",
      "  key=[(17, 25), (21, 29), (25, 33), (29, 37), (33, 41), (37, 45), (41, 49), (45, 53), (49, 57), (53, 61), (57, 65), (61, 69), (65, 73), (69, 77), (73, 81), (77, 85), (81, 89), (85, 93), (89, 97), (93, 101)]\n",
      "  cnt=[2474, 1053, 1076, 971, 869, 730, 594, 470, 386, 335, 266, 199, 137, 116, 105, 92, 63, 37, 17, 10]\n",
      "  batch_size=[64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64]\n",
      "FixedBucketSampler:\n",
      "  sample_num=2999, batch_num=62\n",
      "  key=[(8, 19), (13, 24), (18, 29), (23, 34), (28, 39), (33, 44), (38, 49), (43, 54), (48, 59), (53, 64), (58, 69), (63, 74), (68, 79), (73, 84), (78, 89), (83, 94), (88, 99), (93, 104), (98, 109), (103, 114)]\n",
      "  cnt=[92, 392, 529, 475, 385, 323, 255, 198, 131, 85, 52, 26, 23, 17, 3, 7, 1, 2, 1, 2]\n",
      "  batch_size=[64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64]\n"
     ]
    }
   ],
   "source": [
    "# Samplers\n",
    "wmt_train_batch_sampler = nlp.data.FixedBucketSampler(\n",
    "    lengths=wmt_train_text_with_len.transform(lambda src, tgt, src_len, tgt_len, idx: (src_len, tgt_len)),\n",
    "    num_buckets=hparams.num_buckets,\n",
    "    batch_size=hparams.batch_size)\n",
    "print(wmt_train_batch_sampler.stats())\n",
    "\n",
    "wmt_test_batch_sampler = nlp.data.FixedBucketSampler(\n",
    "    lengths=wmt_test_text_with_len.transform(lambda src, tgt, src_len, tgt_len, idx: (src_len, tgt_len)),\n",
    "    num_buckets=hparams.num_buckets,\n",
    "    batch_size=hparams.test_batch_size)\n",
    "print(wmt_test_batch_sampler.stats())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec987736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training batches: 168\n",
      "Number of testing batches: 62\n"
     ]
    }
   ],
   "source": [
    "# DataLoaders\n",
    "wmt_train_data_loader = mx.gluon.data.DataLoader(\n",
    "    wmt_train_text_with_len,\n",
    "    batch_sampler=wmt_train_batch_sampler,\n",
    "    batchify_fn=wmt_batchify_fn,\n",
    "    num_workers=8)\n",
    "print('Number of training batches:', len(wmt_train_data_loader))\n",
    "\n",
    "wmt_test_data_loader = mx.gluon.data.DataLoader(\n",
    "    wmt_test_text_with_len,\n",
    "    batch_sampler=wmt_test_batch_sampler,\n",
    "    batchify_fn=wmt_batchify_fn,\n",
    "    num_workers=8)\n",
    "print('Number of testing batches:', len(wmt_test_data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d4dbb39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use beam_size=4, alpha=0.6, K=5\n"
     ]
    }
   ],
   "source": [
    "# For Evaluation\n",
    "scorer=nlp.model.BeamSearchScorer(\n",
    "    alpha=hparams.lp_alpha,\n",
    "    K=hparams.lp_k)\n",
    "\n",
    "transformer_translator = nmt.translation.BeamSearchTranslator(\n",
    "    model=transformer_model,\n",
    "    beam_size=hparams.beam_size,\n",
    "    scorer=scorer,\n",
    "    max_length=hparams.max_length)\n",
    "\n",
    "print(\"Use beam_size={}, alpha={}, K={}\".format(hparams.beam_size, hparams.lp_alpha, hparams.lp_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84aaecde",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ba4f55f7d5144188fcecda13b880cf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/62 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extension horovod.torch has not been built: /home/ubuntu/anaconda3/envs/mxnet_p37/lib/python3.7/site-packages/horovod/torch/mpi_lib/_mpi_lib.cpython-37m-x86_64-linux-gnu.so not found\n",
      "If this is not expected, reinstall Horovod with HOROVOD_WITH_PYTORCH=1 to debug the build error.\n",
      "Warning! MPI libs are missing, but python applications are still avaiable.\n",
      "[2022-06-11 11:40:22.772 ip-172-31-28-47:26085 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2022-06-11 11:40:22.802 ip-172-31-28-47:26085 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "WMT16 EN-DE Transformer model test loss: 1.39; test bleu score: 30.19; time cost 183.00s\n"
     ]
    }
   ],
   "source": [
    "# Evaluation (Baseline)\n",
    "eval_start_time = time.time()\n",
    "wmt_loss_function = nlp.loss.MaskedSoftmaxCELoss()\n",
    "wmt_loss_function.hybridize()\n",
    "wmt_detokenizer = nlp.data.SacreMosesDetokenizer()\n",
    "\n",
    "transformer_test_loss, transformer_test_translation_out = nmt.utils.evaluate(\n",
    "    transformer_model,\n",
    "    wmt_test_data_loader,\n",
    "    wmt_loss_function,\n",
    "    transformer_translator,\n",
    "    wmt_tgt_vocab,\n",
    "    wmt_detokenizer,\n",
    "    ctx)\n",
    "\n",
    "transformer_test_bleu_score, _, _, _, _ = nmt.bleu.compute_bleu(\n",
    "    [wmt_test_tgt_sentences],\n",
    "    transformer_test_translation_out,\n",
    "    tokenized=False,\n",
    "    tokenizer=hparams.bleu,\n",
    "    split_compound_word=False,\n",
    "    bpe=False)\n",
    "\n",
    "print('WMT16 EN-DE Transformer model test loss: %.2f; test bleu score: %.2f; time cost %.2fs' %(transformer_test_loss, transformer_test_bleu_score * 100, (time.time() - eval_start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8b75217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "trainer = mx.gluon.Trainer(transformer_model.collect_params(), 'adam', {'learning_rate': hparams.lr})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59d38bb4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d888408759ab4e40a34c862a15afd5ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce627f70ac534834be4d109fb0bd7c5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/168 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 10/168] loss=10.6772, ppl=43355.0385, gnorm=11.6596, throughput=16.98K wps, wc=63.87K\n",
      "[Epoch 0 Batch 20/168] loss=8.3144, ppl=4082.0349, gnorm=1.6977, throughput=28.21K wps, wc=61.03K\n",
      "[Epoch 0 Batch 30/168] loss=7.7640, ppl=2354.2756, gnorm=0.6805, throughput=27.07K wps, wc=53.92K\n",
      "[Epoch 0 Batch 40/168] loss=7.5429, ppl=1887.2953, gnorm=0.4322, throughput=26.81K wps, wc=51.33K\n",
      "[Epoch 0 Batch 50/168] loss=7.4008, ppl=1637.3582, gnorm=0.2987, throughput=27.64K wps, wc=47.70K\n",
      "[Epoch 0 Batch 60/168] loss=7.3407, ppl=1541.7434, gnorm=0.2538, throughput=25.07K wps, wc=42.88K\n",
      "[Epoch 0 Batch 70/168] loss=7.2820, ppl=1453.8531, gnorm=0.2687, throughput=25.32K wps, wc=39.54K\n",
      "[Epoch 0 Batch 80/168] loss=7.2411, ppl=1395.6796, gnorm=0.2621, throughput=24.59K wps, wc=37.88K\n",
      "[Epoch 0 Batch 90/168] loss=7.2312, ppl=1381.9313, gnorm=0.2400, throughput=22.62K wps, wc=34.78K\n",
      "[Epoch 0 Batch 100/168] loss=7.2237, ppl=1371.5189, gnorm=0.2713, throughput=21.69K wps, wc=29.41K\n",
      "[Epoch 0 Batch 110/168] loss=7.1979, ppl=1336.6276, gnorm=0.2388, throughput=21.81K wps, wc=29.80K\n",
      "[Epoch 0 Batch 120/168] loss=7.1093, ppl=1223.2661, gnorm=0.2854, throughput=18.62K wps, wc=25.51K\n",
      "[Epoch 0 Batch 130/168] loss=7.0877, ppl=1197.1112, gnorm=0.2927, throughput=18.69K wps, wc=22.67K\n",
      "[Epoch 0 Batch 140/168] loss=6.9202, ppl=1012.5225, gnorm=0.3935, throughput=13.75K wps, wc=15.68K\n",
      "[Epoch 0 Batch 150/168] loss=7.0375, ppl=1138.5584, gnorm=0.3501, throughput=12.69K wps, wc=16.25K\n",
      "[Epoch 0 Batch 160/168] loss=6.8760, ppl=968.7894, gnorm=0.3260, throughput=13.24K wps, wc=15.62K\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f468340e0d174830ba0be0ce15147913",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/62 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] valid Loss=7.2806, valid ppl=1451.8367, valid bleu=0.00\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ece7bb44b1045ff83829bb2bdfd5358",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/168 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 Batch 10/168] loss=7.4388, ppl=1700.6920, gnorm=0.6407, throughput=22.08K wps, wc=63.87K\n",
      "[Epoch 1 Batch 20/168] loss=7.1792, ppl=1311.8772, gnorm=0.4267, throughput=28.18K wps, wc=61.03K\n",
      "[Epoch 1 Batch 30/168] loss=7.1938, ppl=1331.1514, gnorm=0.3529, throughput=27.35K wps, wc=53.92K\n",
      "[Epoch 1 Batch 40/168] loss=7.1710, ppl=1301.1846, gnorm=0.3638, throughput=26.87K wps, wc=51.33K\n",
      "[Epoch 1 Batch 50/168] loss=7.2138, ppl=1358.0271, gnorm=0.2787, throughput=27.52K wps, wc=47.70K\n",
      "[Epoch 1 Batch 60/168] loss=7.1922, ppl=1328.9878, gnorm=0.2636, throughput=25.05K wps, wc=42.88K\n",
      "[Epoch 1 Batch 70/168] loss=7.1541, ppl=1279.3798, gnorm=0.2775, throughput=25.46K wps, wc=39.54K\n",
      "[Epoch 1 Batch 80/168] loss=7.1264, ppl=1244.4135, gnorm=0.2826, throughput=25.02K wps, wc=37.88K\n",
      "[Epoch 1 Batch 90/168] loss=7.1282, ppl=1246.6253, gnorm=0.2798, throughput=22.97K wps, wc=34.78K\n",
      "[Epoch 1 Batch 100/168] loss=7.0853, ppl=1194.2458, gnorm=0.3322, throughput=21.77K wps, wc=29.41K\n",
      "[Epoch 1 Batch 110/168] loss=7.1289, ppl=1247.5454, gnorm=0.3151, throughput=21.63K wps, wc=29.80K\n",
      "[Epoch 1 Batch 120/168] loss=6.9982, ppl=1094.6604, gnorm=0.3444, throughput=18.68K wps, wc=25.51K\n",
      "[Epoch 1 Batch 130/168] loss=7.0054, ppl=1102.5311, gnorm=0.3644, throughput=18.28K wps, wc=22.67K\n",
      "[Epoch 1 Batch 140/168] loss=6.7707, ppl=871.9642, gnorm=0.4716, throughput=13.49K wps, wc=15.68K\n",
      "[Epoch 1 Batch 150/168] loss=6.8461, ppl=940.1728, gnorm=0.4790, throughput=12.75K wps, wc=16.25K\n",
      "[Epoch 1 Batch 160/168] loss=6.7619, ppl=864.3241, gnorm=0.4713, throughput=14.11K wps, wc=15.62K\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cc9c58c4648487d8fa2e6166908e960",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/62 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] valid Loss=7.6062, valid ppl=2010.7079, valid bleu=0.00\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69f7117c14d54bcbb75c65d933d69a1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/168 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2 Batch 10/168] loss=7.4394, ppl=1701.7766, gnorm=0.5685, throughput=21.74K wps, wc=63.87K\n",
      "[Epoch 2 Batch 20/168] loss=7.1701, ppl=1299.9511, gnorm=0.3847, throughput=28.02K wps, wc=61.03K\n",
      "[Epoch 2 Batch 30/168] loss=7.2013, ppl=1341.2192, gnorm=0.3120, throughput=26.13K wps, wc=53.92K\n",
      "[Epoch 2 Batch 40/168] loss=7.1982, ppl=1337.0268, gnorm=0.3102, throughput=27.05K wps, wc=51.33K\n",
      "[Epoch 2 Batch 50/168] loss=7.1619, ppl=1289.3603, gnorm=0.2758, throughput=27.31K wps, wc=47.70K\n",
      "[Epoch 2 Batch 60/168] loss=7.1761, ppl=1307.7469, gnorm=0.2501, throughput=25.10K wps, wc=42.88K\n",
      "[Epoch 2 Batch 70/168] loss=7.1083, ppl=1222.1133, gnorm=0.2727, throughput=25.44K wps, wc=39.54K\n",
      "[Epoch 2 Batch 80/168] loss=7.0957, ppl=1206.8085, gnorm=0.2598, throughput=25.05K wps, wc=37.88K\n",
      "[Epoch 2 Batch 90/168] loss=7.0851, ppl=1194.0367, gnorm=0.2776, throughput=23.85K wps, wc=34.78K\n",
      "[Epoch 2 Batch 100/168] loss=7.0661, ppl=1171.5677, gnorm=0.3305, throughput=20.38K wps, wc=29.41K\n",
      "[Epoch 2 Batch 110/168] loss=7.0703, ppl=1176.5573, gnorm=0.3425, throughput=22.21K wps, wc=29.80K\n",
      "[Epoch 2 Batch 120/168] loss=6.9954, ppl=1091.5802, gnorm=0.3686, throughput=19.80K wps, wc=25.51K\n",
      "[Epoch 2 Batch 130/168] loss=6.9591, ppl=1052.7071, gnorm=0.3767, throughput=17.44K wps, wc=22.67K\n",
      "[Epoch 2 Batch 140/168] loss=6.7510, ppl=854.9031, gnorm=0.5102, throughput=13.58K wps, wc=15.68K\n",
      "[Epoch 2 Batch 150/168] loss=6.8315, ppl=926.5612, gnorm=0.5356, throughput=13.79K wps, wc=16.25K\n",
      "[Epoch 2 Batch 160/168] loss=6.7307, ppl=837.7258, gnorm=0.4945, throughput=13.56K wps, wc=15.62K\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0bb3cf13cfe400299fd7e058636d53b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/62 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] valid Loss=7.5910, valid ppl=1980.3742, valid bleu=0.00\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63164209ab054849a739c36a91868bc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/168 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3 Batch 10/168] loss=7.4335, ppl=1691.7161, gnorm=0.7444, throughput=21.26K wps, wc=63.87K\n",
      "[Epoch 3 Batch 20/168] loss=7.1750, ppl=1306.4083, gnorm=0.4588, throughput=25.31K wps, wc=61.03K\n",
      "[Epoch 3 Batch 30/168] loss=7.1713, ppl=1301.5784, gnorm=0.3152, throughput=26.38K wps, wc=53.92K\n",
      "[Epoch 3 Batch 40/168] loss=7.1452, ppl=1267.9969, gnorm=0.3600, throughput=26.07K wps, wc=51.33K\n",
      "[Epoch 3 Batch 50/168] loss=7.1685, ppl=1297.9416, gnorm=0.3108, throughput=26.10K wps, wc=47.70K\n",
      "[Epoch 3 Batch 60/168] loss=7.1235, ppl=1240.7333, gnorm=0.2645, throughput=24.37K wps, wc=42.88K\n",
      "[Epoch 3 Batch 70/168] loss=7.0959, ppl=1206.9974, gnorm=0.2830, throughput=25.20K wps, wc=39.54K\n",
      "[Epoch 3 Batch 80/168] loss=7.0751, ppl=1182.1475, gnorm=0.2838, throughput=23.13K wps, wc=37.88K\n",
      "[Epoch 3 Batch 90/168] loss=7.0670, ppl=1172.6012, gnorm=0.2836, throughput=23.64K wps, wc=34.78K\n",
      "[Epoch 3 Batch 100/168] loss=7.0468, ppl=1149.2247, gnorm=0.3261, throughput=20.30K wps, wc=29.41K\n",
      "[Epoch 3 Batch 110/168] loss=7.0378, ppl=1138.8973, gnorm=0.3843, throughput=21.33K wps, wc=29.80K\n",
      "[Epoch 3 Batch 120/168] loss=6.9657, ppl=1059.6207, gnorm=0.3449, throughput=18.40K wps, wc=25.51K\n",
      "[Epoch 3 Batch 130/168] loss=6.9166, ppl=1008.8701, gnorm=0.4033, throughput=18.18K wps, wc=22.67K\n",
      "[Epoch 3 Batch 140/168] loss=6.7027, ppl=814.5975, gnorm=0.5327, throughput=12.49K wps, wc=15.68K\n",
      "[Epoch 3 Batch 150/168] loss=6.7948, ppl=893.2260, gnorm=0.5674, throughput=14.06K wps, wc=16.25K\n",
      "[Epoch 3 Batch 160/168] loss=6.6848, ppl=800.1878, gnorm=0.5158, throughput=14.00K wps, wc=15.62K\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "462bbd86c6494c1bb19382761bb2663f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/62 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-4126e02a901d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mhparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mhparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     ctx)\n\u001b[0m",
      "\u001b[0;32m~/code/packt/Deep-Learning-with-MXNet-Cookbook/ch06/nmt/utils.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_data_loader, valid_data_loader, loss_function, trainer, tgt_vocab, val_tgt_sentences, detokenizer, file_name, hparams, ctx)\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0mtgt_vocab\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0mdetokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m             ctx)\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         valid_bleu_score, _, _, _, _ = bleu.compute_bleu(\n",
      "\u001b[0;32m~/code/packt/Deep-Learning-with-MXNet-Cookbook/ch06/nmt/utils.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, data_loader, test_loss_function, translator, tgt_vocab, detokenizer, context)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;31m# Translate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_valid_length\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0mtranslator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_seq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_valid_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_valid_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0mmax_score_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0msample_valid_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_valid_length\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/packt/Deep-Learning-with-MXNet-Cookbook/ch06/nmt/translation.py\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(self, src_seq, src_valid_length)\u001b[0m\n\u001b[1;32m     79\u001b[0m         inputs = mx.nd.full(shape=(batch_size,), ctx=src_seq.context, dtype=np.float32,\n\u001b[1;32m     80\u001b[0m                             val=self._model.tgt_vocab.token_to_idx[self._model.tgt_vocab.bos_token])\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_valid_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_valid_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/gluonnlp/model/sequence_sampler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, states)\u001b[0m\n\u001b[1;32m    546\u001b[0m                               new_states, vocab_size_nd, batch_shift_nd)\n\u001b[1;32m    547\u001b[0m             \u001b[0mstep_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchosen_word_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mmx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeam_alive_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    549\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m         final_word = mx.nd.where(beam_alive_mask,\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/mxnet_p37/gpu_cuda11.0/lib/python3.7/site-packages/mxnet/ndarray/ndarray.py\u001b[0m in \u001b[0;36masscalar\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2583\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The current array is not a scalar\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2584\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2585\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2586\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2587\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/mxnet_p37/gpu_cuda11.0/lib/python3.7/site-packages/mxnet/ndarray/ndarray.py\u001b[0m in \u001b[0;36masnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2564\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2565\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2566\u001b[0;31m             ctypes.c_size_t(data.size)))\n\u001b[0m\u001b[1;32m   2567\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hparams.epochs = 10\n",
    "\n",
    "test_loss, test_translation_out = nmt.utils.train(\n",
    "    transformer_model,\n",
    "    wmt_train_data_loader,\n",
    "    wmt_test_data_loader,\n",
    "    wmt_loss_function,\n",
    "    trainer,\n",
    "    # transformer_translator,\n",
    "    wmt_tgt_vocab,\n",
    "    wmt_test_tgt_sentences,\n",
    "    wmt_detokenizer,\n",
    "    hparams.file_name,\n",
    "    hparams,\n",
    "    ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba712ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e417311f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0966d412",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5186a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "\n",
    "reload(nmt)\n",
    "reload(nmt.utils)\n",
    "reload(nmt.transformer_hparams)\n",
    "\n",
    "hparams = nmt.transformer_hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d26a34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846120a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7455d88b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b1f04a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654d9b57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2deba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Qualitative Evaluation: Translating from English to German:\")\n",
    "\n",
    "sample_src_seq = \"I love reading technical books.\"\n",
    "print(\"[\\'\" + sample_src_seq + \"\\']\")\n",
    "\n",
    "sample_tgt_seq = nmt.utils.translate(\n",
    "    transformer_translator,\n",
    "    sample_src_seq,\n",
    "    wmt_src_vocab,\n",
    "    wmt_tgt_vocab,\n",
    "    wmt_detokenizer,\n",
    "    ctx)\n",
    "\n",
    "print(\"The German translation is:\")\n",
    "print(sample_tgt_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf630d3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b68750",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6dc7a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8f3254",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4a046f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Sample target sentence: \"{}\"'.format(wmt_test_tgt_sentences[16]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5130b82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e79a7d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad261e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(transformer_test_translation_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e046d866",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(wmt_test_tgt_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9272b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b469f6a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c04699",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d1053f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f877eeae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879fdb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Evaluation\n",
    "scorer=nlp.model.BeamSearchScorer(\n",
    "    alpha=hparams.lp_alpha,\n",
    "    K=hparams.lp_k)\n",
    "\n",
    "transformer_translator = nmt.translation.BeamSearchTranslator(\n",
    "    model=transformer_model,\n",
    "    beam_size=hparams.beam_size,\n",
    "    scorer=scorer,\n",
    "    max_length=tgt_max_len + 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f139de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation (Baseline)\n",
    "eval_start_time = time.time()\n",
    "wmt_loss_function = nlp.loss.MaskedSoftmaxCELoss()\n",
    "wmt_loss_function.hybridize()\n",
    "wmt_detokenizer = nlp.data.SacreMosesDetokenizer()\n",
    "\n",
    "transformer_test_loss, transformer_test_translation_out = nmt.utils.evaluate(\n",
    "    transformer_model,\n",
    "    wmt_test_data_loader,\n",
    "    wmt_loss_function,\n",
    "    transformer_translator,\n",
    "    wmt_tgt_vocab,\n",
    "    wmt_detokenizer,\n",
    "    ctx)\n",
    "\n",
    "transformer_test_bleu_score, _, _, _, _ = nmt.bleu.compute_bleu(\n",
    "    [wmt_test_tgt_sentences],\n",
    "    transformer_test_translation_out,\n",
    "    tokenized=False,\n",
    "    tokenizer=hparams.bleu,\n",
    "    split_compound_word=False,\n",
    "    bpe=False)\n",
    "\n",
    "print('WMT16 EN-DE Transformer model test loss: %.2f; test bleu score: %.2f; time cost %.2fs' %(transformer_test_loss, transformer_test_bleu_score * 100, (time.time() - eval_start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e1ca09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
